{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('waitlist_kidney_brazil_prepared.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_registered</th>\n",
       "      <th>age_registered</th>\n",
       "      <th>dialysis_session_count</th>\n",
       "      <th>sex</th>\n",
       "      <th>underlying_disease</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>chagas</th>\n",
       "      <th>blood_type</th>\n",
       "      <th>transfusion_count</th>\n",
       "      <th>gestation</th>\n",
       "      <th>prior_transplant</th>\n",
       "      <th>c_pra</th>\n",
       "      <th>hla_a1</th>\n",
       "      <th>hla_a2</th>\n",
       "      <th>hla_b1</th>\n",
       "      <th>hla_b2</th>\n",
       "      <th>hla_dr1</th>\n",
       "      <th>hla_dr2</th>\n",
       "      <th>dr_00</th>\n",
       "      <th>b_00</th>\n",
       "      <th>a_00</th>\n",
       "      <th>anti_hbc</th>\n",
       "      <th>anti_hcv</th>\n",
       "      <th>hbs_ag</th>\n",
       "      <th>event</th>\n",
       "      <th>days_waiting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "      <td>other</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>waiting</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-11-03</td>\n",
       "      <td>58</td>\n",
       "      <td>4.0</td>\n",
       "      <td>M</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>homozygous</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>died_waiting</td>\n",
       "      <td>2066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-07-13</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>M</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>removed</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-10-03</td>\n",
       "      <td>52</td>\n",
       "      <td>17.0</td>\n",
       "      <td>M</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>removed</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-07-05</td>\n",
       "      <td>67</td>\n",
       "      <td>68.0</td>\n",
       "      <td>M</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>heterozygous</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>died_waiting</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_registered  age_registered  dialysis_session_count sex  \\\n",
       "0      2017-06-29              67                     1.0   M   \n",
       "1      2008-11-03              58                     4.0   M   \n",
       "2      2010-07-13              51                     2.0   M   \n",
       "3      2011-10-03              52                    17.0   M   \n",
       "4      2006-07-05              67                    68.0   M   \n",
       "\n",
       "  underlying_disease  diabetes  chagas blood_type  transfusion_count  \\\n",
       "0              other      True   False          A                  0   \n",
       "1           diabetes     False   False          A                  0   \n",
       "2       hypertension      True   False          O                  0   \n",
       "3           diabetes     False   False          O                  0   \n",
       "4       hypertension      True   False          A                  0   \n",
       "\n",
       "   gestation  prior_transplant  c_pra  hla_a1  hla_a2  hla_b1  hla_b2  \\\n",
       "0      False             False      0       1      26      44      51   \n",
       "1      False             False      0       1      24      18      35   \n",
       "2      False             False     64      24      25      14      18   \n",
       "3      False              True      2      24      25      14      18   \n",
       "4      False             False      0      24      68      14      27   \n",
       "\n",
       "   hla_dr1  hla_dr2         dr_00          b_00          a_00  anti_hbc  \\\n",
       "0        3        7  heterozygous  heterozygous  heterozygous     False   \n",
       "1       11        0    homozygous  heterozygous  heterozygous     False   \n",
       "2        1       15  heterozygous  heterozygous  heterozygous     False   \n",
       "3        1       15  heterozygous  heterozygous  heterozygous     False   \n",
       "4       13       15  heterozygous  heterozygous  heterozygous     False   \n",
       "\n",
       "   anti_hcv  hbs_ag         event  days_waiting  \n",
       "0     False   False       waiting           392  \n",
       "1     False   False  died_waiting          2066  \n",
       "2     False   False       removed           365  \n",
       "3     False   False       removed           365  \n",
       "4     False   False  died_waiting           194  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_registered'] = pd.to_datetime(df['date_registered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df#.drop(columns=['date_registered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event\n",
       "removed         14356\n",
       "transplanted    13732\n",
       "waiting         10933\n",
       "died_waiting     9132\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['event'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos os dados com evento `waiting` ou `removed`, porque representam casos em que um transplante n√£o aconteceu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event\n",
       "transplanted    13732\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean[df_clean['event'] == 'transplanted']\n",
    "df_clean['event'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos a coluna de evento, que passa a ser redundante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(columns=['event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['date_transplanted'] = df_clean['date_registered'] + pd.to_timedelta(df_clean['days_waiting'], 'days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['date_registered'] = df_clean['date_registered'].astype(int) / (1e9 * 3600 * 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_registered</th>\n",
       "      <th>age_registered</th>\n",
       "      <th>dialysis_session_count</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>chagas</th>\n",
       "      <th>transfusion_count</th>\n",
       "      <th>gestation</th>\n",
       "      <th>prior_transplant</th>\n",
       "      <th>c_pra</th>\n",
       "      <th>hla_a1</th>\n",
       "      <th>hla_a2</th>\n",
       "      <th>hla_b1</th>\n",
       "      <th>hla_b2</th>\n",
       "      <th>hla_dr1</th>\n",
       "      <th>hla_dr2</th>\n",
       "      <th>anti_hbc</th>\n",
       "      <th>anti_hcv</th>\n",
       "      <th>hbs_ag</th>\n",
       "      <th>days_waiting</th>\n",
       "      <th>date_transplanted</th>\n",
       "      <th>sex_M</th>\n",
       "      <th>underlying_disease_glomerulonephritis</th>\n",
       "      <th>underlying_disease_hypertension</th>\n",
       "      <th>underlying_disease_other</th>\n",
       "      <th>underlying_disease_pyelonephritis</th>\n",
       "      <th>blood_type_AB</th>\n",
       "      <th>blood_type_B</th>\n",
       "      <th>blood_type_O</th>\n",
       "      <th>dr_00_homozygous</th>\n",
       "      <th>b_00_homozygous</th>\n",
       "      <th>a_00_homozygous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16310.0</td>\n",
       "      <td>68</td>\n",
       "      <td>14.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>136</td>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15160.0</td>\n",
       "      <td>62</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>268</td>\n",
       "      <td>2012-03-29</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12962.0</td>\n",
       "      <td>47</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1345</td>\n",
       "      <td>2009-03-04</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13424.0</td>\n",
       "      <td>65</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1283</td>\n",
       "      <td>2010-04-08</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15525.0</td>\n",
       "      <td>33</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>464</td>\n",
       "      <td>2013-10-11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date_registered  age_registered  dialysis_session_count  diabetes  chagas  \\\n",
       "12          16310.0              68                    14.0     False   False   \n",
       "14          15160.0              62                    12.0      True   False   \n",
       "15          12962.0              47                     1.0      True   False   \n",
       "27          13424.0              65                     7.0      True   False   \n",
       "28          15525.0              33                    12.0      True   False   \n",
       "\n",
       "    transfusion_count  gestation  prior_transplant  c_pra  hla_a1  hla_a2  \\\n",
       "12                  1      False             False      0       2       3   \n",
       "14                  0      False             False      0       2      68   \n",
       "15                  0      False             False      0       1       2   \n",
       "27                  0      False             False      0       2      29   \n",
       "28                  0      False             False      0       2      23   \n",
       "\n",
       "    hla_b1  hla_b2  hla_dr1  hla_dr2  anti_hbc  anti_hcv  hbs_ag  \\\n",
       "12      14      51        1       13     False     False   False   \n",
       "14      39      40        4        0     False     False   False   \n",
       "15      40      57        4        4     False     False   False   \n",
       "27      44       0        7       11     False     False   False   \n",
       "28       8      44        8       15     False     False   False   \n",
       "\n",
       "    days_waiting date_transplanted  sex_M  \\\n",
       "12           136        2015-01-11   True   \n",
       "14           268        2012-03-29   True   \n",
       "15          1345        2009-03-04   True   \n",
       "27          1283        2010-04-08   True   \n",
       "28           464        2013-10-11  False   \n",
       "\n",
       "    underlying_disease_glomerulonephritis  underlying_disease_hypertension  \\\n",
       "12                                  False                            False   \n",
       "14                                  False                            False   \n",
       "15                                  False                            False   \n",
       "27                                  False                            False   \n",
       "28                                   True                            False   \n",
       "\n",
       "    underlying_disease_other  underlying_disease_pyelonephritis  \\\n",
       "12                     False                              False   \n",
       "14                      True                              False   \n",
       "15                      True                              False   \n",
       "27                      True                              False   \n",
       "28                     False                              False   \n",
       "\n",
       "    blood_type_AB  blood_type_B  blood_type_O  dr_00_homozygous  \\\n",
       "12          False          True         False             False   \n",
       "14           True         False         False              True   \n",
       "15          False         False          True             False   \n",
       "27          False         False         False             False   \n",
       "28          False         False         False             False   \n",
       "\n",
       "    b_00_homozygous  a_00_homozygous  \n",
       "12            False            False  \n",
       "14            False            False  \n",
       "15            False            False  \n",
       "27             True            False  \n",
       "28            False            False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onehot = pd.get_dummies(df_clean, columns=['sex', 'underlying_disease', 'blood_type', 'dr_00', 'b_00', 'a_00'], drop_first=True)\n",
    "df_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot['idx'] = df_onehot.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df_onehot.shape[0]\n",
    "N_train = N * 80 // 100\n",
    "\n",
    "df_onehot.sort_values(by='date_registered', inplace=True, ignore_index=True)\n",
    "df_onehot, df_test = df_onehot.iloc[:N_train], df_onehot[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1729)\n",
    "\n",
    "pairs = set()\n",
    "while len(pairs) < 200000:\n",
    "    a = random.randint(1, df_onehot.shape[0] - 1)\n",
    "    b = random.randint(max(0, a - 1000), a-1)\n",
    "    ra = df_onehot.iloc[a]\n",
    "    rb = df_onehot.iloc[b]\n",
    "    for c in df_onehot.columns:\n",
    "        if ra[c] < rb[c]:\n",
    "            pairs.add((a,b))\n",
    "            break\n",
    "        if rb[c] < ra[c]:\n",
    "            pairs.add((b,a))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_registered_a</th>\n",
       "      <th>age_registered_a</th>\n",
       "      <th>dialysis_session_count_a</th>\n",
       "      <th>diabetes_a</th>\n",
       "      <th>chagas_a</th>\n",
       "      <th>transfusion_count_a</th>\n",
       "      <th>gestation_a</th>\n",
       "      <th>prior_transplant_a</th>\n",
       "      <th>c_pra_a</th>\n",
       "      <th>hla_a1_a</th>\n",
       "      <th>hla_a2_a</th>\n",
       "      <th>hla_b1_a</th>\n",
       "      <th>hla_b2_a</th>\n",
       "      <th>hla_dr1_a</th>\n",
       "      <th>hla_dr2_a</th>\n",
       "      <th>anti_hbc_a</th>\n",
       "      <th>anti_hcv_a</th>\n",
       "      <th>hbs_ag_a</th>\n",
       "      <th>days_waiting_a</th>\n",
       "      <th>date_transplanted_a</th>\n",
       "      <th>sex_M_a</th>\n",
       "      <th>underlying_disease_glomerulonephritis_a</th>\n",
       "      <th>underlying_disease_hypertension_a</th>\n",
       "      <th>underlying_disease_other_a</th>\n",
       "      <th>underlying_disease_pyelonephritis_a</th>\n",
       "      <th>blood_type_AB_a</th>\n",
       "      <th>blood_type_B_a</th>\n",
       "      <th>blood_type_O_a</th>\n",
       "      <th>dr_00_homozygous_a</th>\n",
       "      <th>b_00_homozygous_a</th>\n",
       "      <th>a_00_homozygous_a</th>\n",
       "      <th>idx_a</th>\n",
       "      <th>date_registered_b</th>\n",
       "      <th>age_registered_b</th>\n",
       "      <th>dialysis_session_count_b</th>\n",
       "      <th>diabetes_b</th>\n",
       "      <th>chagas_b</th>\n",
       "      <th>transfusion_count_b</th>\n",
       "      <th>gestation_b</th>\n",
       "      <th>prior_transplant_b</th>\n",
       "      <th>c_pra_b</th>\n",
       "      <th>hla_a1_b</th>\n",
       "      <th>hla_a2_b</th>\n",
       "      <th>hla_b1_b</th>\n",
       "      <th>hla_b2_b</th>\n",
       "      <th>hla_dr1_b</th>\n",
       "      <th>hla_dr2_b</th>\n",
       "      <th>anti_hbc_b</th>\n",
       "      <th>anti_hcv_b</th>\n",
       "      <th>hbs_ag_b</th>\n",
       "      <th>days_waiting_b</th>\n",
       "      <th>date_transplanted_b</th>\n",
       "      <th>sex_M_b</th>\n",
       "      <th>underlying_disease_glomerulonephritis_b</th>\n",
       "      <th>underlying_disease_hypertension_b</th>\n",
       "      <th>underlying_disease_other_b</th>\n",
       "      <th>underlying_disease_pyelonephritis_b</th>\n",
       "      <th>blood_type_AB_b</th>\n",
       "      <th>blood_type_B_b</th>\n",
       "      <th>blood_type_O_b</th>\n",
       "      <th>dr_00_homozygous_b</th>\n",
       "      <th>b_00_homozygous_b</th>\n",
       "      <th>a_00_homozygous_b</th>\n",
       "      <th>idx_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13449.0</td>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2701</td>\n",
       "      <td>2014-03-21</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>24958</td>\n",
       "      <td>13453.0</td>\n",
       "      <td>31</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>66</td>\n",
       "      <td>40</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1054</td>\n",
       "      <td>2009-09-20</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>35188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13614.0</td>\n",
       "      <td>53</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>68</td>\n",
       "      <td>27</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1472</td>\n",
       "      <td>2011-04-22</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>43722</td>\n",
       "      <td>13700.0</td>\n",
       "      <td>36</td>\n",
       "      <td>87.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>51</td>\n",
       "      <td>53</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1750</td>\n",
       "      <td>2012-04-20</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>29660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14840.0</td>\n",
       "      <td>54</td>\n",
       "      <td>46.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>45</td>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>374</td>\n",
       "      <td>2011-08-28</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>26316</td>\n",
       "      <td>14988.0</td>\n",
       "      <td>45</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>480</td>\n",
       "      <td>2012-05-08</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>27848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13830.0</td>\n",
       "      <td>41</td>\n",
       "      <td>15.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>423</td>\n",
       "      <td>2009-01-09</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>41851</td>\n",
       "      <td>13984.0</td>\n",
       "      <td>57</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>74</td>\n",
       "      <td>42</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>948</td>\n",
       "      <td>2010-11-19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13558.0</td>\n",
       "      <td>37</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1077</td>\n",
       "      <td>2010-01-26</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>20527</td>\n",
       "      <td>13721.0</td>\n",
       "      <td>51</td>\n",
       "      <td>26.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1630</td>\n",
       "      <td>2012-01-12</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>27765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_registered_a  age_registered_a  dialysis_session_count_a  diabetes_a  \\\n",
       "0            13449.0                19                       4.0        True   \n",
       "1            13614.0                53                       5.0        True   \n",
       "2            14840.0                54                      46.0        True   \n",
       "3            13830.0                41                      15.0        True   \n",
       "4            13558.0                37                      10.0        True   \n",
       "\n",
       "   chagas_a  transfusion_count_a  gestation_a  prior_transplant_a  c_pra_a  \\\n",
       "0     False                    0        False               False        0   \n",
       "1     False                    0        False               False        0   \n",
       "2     False                    1        False               False        0   \n",
       "3     False                    0        False               False        0   \n",
       "4     False                    0        False               False        0   \n",
       "\n",
       "   hla_a1_a  hla_a2_a  hla_b1_a  hla_b2_a  hla_dr1_a  hla_dr2_a  anti_hbc_a  \\\n",
       "0        23        66        15        44         12         13       False   \n",
       "1        24        68        27        51          3         10       False   \n",
       "2         3        29        45        52         11         14        True   \n",
       "3         2         0        39        49          1          8       False   \n",
       "4         3         0        15        49          1         10       False   \n",
       "\n",
       "   anti_hcv_a  hbs_ag_a  days_waiting_a date_transplanted_a  sex_M_a  \\\n",
       "0       False     False            2701          2014-03-21     True   \n",
       "1       False     False            1472          2011-04-22    False   \n",
       "2       False     False             374          2011-08-28     True   \n",
       "3       False     False             423          2009-01-09    False   \n",
       "4       False     False            1077          2010-01-26     True   \n",
       "\n",
       "   underlying_disease_glomerulonephritis_a  underlying_disease_hypertension_a  \\\n",
       "0                                    False                              False   \n",
       "1                                    False                              False   \n",
       "2                                    False                               True   \n",
       "3                                    False                              False   \n",
       "4                                    False                               True   \n",
       "\n",
       "   underlying_disease_other_a  underlying_disease_pyelonephritis_a  \\\n",
       "0                        True                                False   \n",
       "1                        True                                False   \n",
       "2                       False                                False   \n",
       "3                        True                                False   \n",
       "4                       False                                False   \n",
       "\n",
       "   blood_type_AB_a  blood_type_B_a  blood_type_O_a  dr_00_homozygous_a  \\\n",
       "0            False            True           False               False   \n",
       "1            False           False            True               False   \n",
       "2            False           False            True               False   \n",
       "3            False           False           False               False   \n",
       "4            False           False           False               False   \n",
       "\n",
       "   b_00_homozygous_a  a_00_homozygous_a  idx_a  date_registered_b  \\\n",
       "0              False              False  24958            13453.0   \n",
       "1              False              False  43722            13700.0   \n",
       "2              False              False  26316            14988.0   \n",
       "3              False               True  41851            13984.0   \n",
       "4              False               True  20527            13721.0   \n",
       "\n",
       "   age_registered_b  dialysis_session_count_b  diabetes_b  chagas_b  \\\n",
       "0                31                       6.0        True     False   \n",
       "1                36                      87.0        True     False   \n",
       "2                45                      10.0        True     False   \n",
       "3                57                      10.0       False     False   \n",
       "4                51                      26.0        True     False   \n",
       "\n",
       "   transfusion_count_b  gestation_b  prior_transplant_b  c_pra_b  hla_a1_b  \\\n",
       "0                    1        False               False        0        31   \n",
       "1                    0        False                True        0        31   \n",
       "2                    0        False               False        0         2   \n",
       "3                    0        False               False        0        30   \n",
       "4                    1        False               False        0         3   \n",
       "\n",
       "   hla_a2_b  hla_b1_b  hla_b2_b  hla_dr1_b  hla_dr2_b  anti_hbc_b  anti_hcv_b  \\\n",
       "0        66        40        58          8         14       False       False   \n",
       "1        36        51        53          8         11       False       False   \n",
       "2        33        15        53          1         16       False       False   \n",
       "3        74        42        53          3         11       False       False   \n",
       "4        32        27        38         13         15       False       False   \n",
       "\n",
       "   hbs_ag_b  days_waiting_b date_transplanted_b  sex_M_b  \\\n",
       "0     False            1054          2009-09-20     True   \n",
       "1     False            1750          2012-04-20    False   \n",
       "2     False             480          2012-05-08     True   \n",
       "3     False             948          2010-11-19     True   \n",
       "4     False            1630          2012-01-12     True   \n",
       "\n",
       "   underlying_disease_glomerulonephritis_b  underlying_disease_hypertension_b  \\\n",
       "0                                    False                               True   \n",
       "1                                    False                               True   \n",
       "2                                    False                               True   \n",
       "3                                    False                              False   \n",
       "4                                     True                              False   \n",
       "\n",
       "   underlying_disease_other_b  underlying_disease_pyelonephritis_b  \\\n",
       "0                       False                                False   \n",
       "1                       False                                False   \n",
       "2                       False                                False   \n",
       "3                       False                                False   \n",
       "4                       False                                False   \n",
       "\n",
       "   blood_type_AB_b  blood_type_B_b  blood_type_O_b  dr_00_homozygous_b  \\\n",
       "0            False           False           False               False   \n",
       "1            False           False            True               False   \n",
       "2            False            True           False               False   \n",
       "3            False            True           False               False   \n",
       "4            False            True           False               False   \n",
       "\n",
       "   b_00_homozygous_b  a_00_homozygous_b  idx_b  \n",
       "0              False              False  35188  \n",
       "1              False              False  29660  \n",
       "2              False              False  27848  \n",
       "3              False              False    168  \n",
       "4              False              False  27765  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = []\n",
    "for _a, _b in pairs:\n",
    "    a = df_onehot.iloc[_a]\n",
    "    b = df_onehot.iloc[_b]\n",
    "    a.index = a.index + '_a'\n",
    "    b.index = b.index + '_b'\n",
    "    s.append(pd.concat((a,b)))\n",
    "df_cross = pd.DataFrame(s)\n",
    "df_cross.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross['transplanted_first'] = df_cross['date_transplanted_a'] < df_cross['date_transplanted_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     200000\n",
       "unique         2\n",
       "top         True\n",
       "freq      115428\n",
       "Name: transplanted_first, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross['transplanted_first'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross_cln = df_cross #.drop(columns=['days_waiting_a', 'days_waiting_b', 'transplant_date_a', 'transplant_date_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'transplanted_first'\n",
    "_meta = ['idx_a', 'idx_b','days_waiting_a', 'days_waiting_b', 'date_transplanted_a', 'date_transplanted_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, meta = df_cross_cln.drop(columns=[target, *_meta]), df_cross_cln[target], df_cross_cln[_meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(X, y, meta, test_size=0.2, random_state=1729)\n",
    "\n",
    "N = df_cross_cln.shape[0]\n",
    "N_train = N * 80 // 100\n",
    "\n",
    "X_train, X_test = X.iloc[:N_train], X.iloc[N_train:]\n",
    "y_train, y_test = y.iloc[:N_train], y.iloc[N_train:]\n",
    "meta_train, meta_test = meta.iloc[:N_train], meta.iloc[N_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M√©tricas de avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plug-in\n",
    "\n",
    "Modelo de QDA\n",
    "N√£o tem hiperpar√¢metros, mas √© o modelo de separa√ß√£o simples plug-in que a gente conhece ¬Ø\\\\_(„ÉÑ)_/¬Ø\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>QuadraticDiscriminantAnalysis()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">QuadraticDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>QuadraticDiscriminantAnalysis()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "QuadraticDiscriminantAnalysis()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.577844396793264, 0.6049)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_qda = qda.predict(X_test)\n",
    "(roc_auc_score(y_test, y_test_qda), accuracy_score(y_test, y_test_qda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "His ass DID NOT learn shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5904998924799905, 0.61925)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_qda = qda.predict(X_train)\n",
    "(roc_auc_score(y_train, y_train_qda), accuracy_score(y_train, y_train_qda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk-minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Modelo de SVC Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 4444\n",
      "max_resources_: 40000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 12\n",
      "n_resources: 4444\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.665, test=0.615) total time=   0.5s\n",
      "[CV 4/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.630, test=0.607) total time=   0.5s\n",
      "[CV 1/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.652, test=0.617) total time=   0.1s\n",
      "[CV 1/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.652, test=0.617) total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END linearsvc__C=1, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.665, test=0.615) total time=   0.4s\n",
      "[CV 1/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.652, test=0.617) total time=   0.5s\n",
      "[CV 5/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.655, test=0.610) total time=   0.6s\n",
      "[CV 2/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.639, test=0.652) total time=   0.1s\n",
      "[CV 2/5] END linearsvc__C=1000, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.639, test=0.651) total time=   0.5s\n",
      "[CV 2/5] END linearsvc__C=1, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.634, test=0.642) total time=   0.4s\n",
      "[CV 4/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.630, test=0.607) total time=   0.5s\n",
      "[CV 4/5] END linearsvc__C=1, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.620, test=0.609) total time=   0.4s\n",
      "[CV 3/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.665, test=0.615) total time=   0.7s\n",
      "[CV 2/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.639, test=0.651) total time=   0.6s\n",
      "[CV 3/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.665, test=0.615) total time=   0.5s\n",
      "[CV 4/5] END linearsvc__C=1000, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.630, test=0.607) total time=   0.5s\n",
      "[CV 5/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.655, test=0.610) total time=   0.5s\n",
      "[CV 4/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.630, test=0.607) total time=   0.1s\n",
      "[CV 1/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.652, test=0.617) total time=   0.7s\n",
      "[CV 1/5] END linearsvc__C=1, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.648, test=0.617) total time=   0.5s[CV 3/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.665, test=0.615) total time=   0.1s\n",
      "\n",
      "[CV 3/5] END linearsvc__C=1000, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.665, test=0.615) total time=   0.5s\n",
      "[CV 5/5] END linearsvc__C=1000, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.655, test=0.610) total time=   0.5s\n",
      "[CV 5/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.655, test=0.610) total time=   0.1s\n",
      "[CV 1/5] END linearsvc__C=1000, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.652, test=0.617) total time=   0.6s\n",
      "[CV 5/5] END linearsvc__C=1, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.653, test=0.605) total time=   0.3s\n",
      "[CV 3/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.665, test=0.615) total time=   0.1s\n",
      "[CV 1/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.652, test=0.617) total time=   0.1s\n",
      "[CV 2/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.639, test=0.651) total time=   0.1s\n",
      "[CV 4/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.630, test=0.607) total time=   0.1s\n",
      "[CV 4/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.630, test=0.607) total time=   0.8s\n",
      "[CV 5/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.655, test=0.610) total time=   0.1s\n",
      "[CV 2/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.639, test=0.651) total time=   0.1s\n",
      "[CV 4/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.630, test=0.607) total time=   0.1s\n",
      "[CV 1/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.652, test=0.617) total time=   0.1s\n",
      "[CV 3/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.665, test=0.615) total time=   0.1s\n",
      "[CV 5/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.655, test=0.610) total time=   0.1s\n",
      "[CV 2/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.639, test=0.652) total time=   1.0s\n",
      "[CV 1/5] END linearsvc__C=1000, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.652, test=0.617) total time=   0.1s\n",
      "[CV 2/5] END linearsvc__C=1000, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.639, test=0.651) total time=   0.1s\n",
      "[CV 3/5] END linearsvc__C=1000, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.665, test=0.615) total time=   0.1s\n",
      "[CV 4/5] END linearsvc__C=1000, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.630, test=0.607) total time=   0.1s\n",
      "[CV 5/5] END linearsvc__C=1000, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.655, test=0.610) total time=   0.1s\n",
      "[CV 5/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.655, test=0.610) total time=   1.1s\n",
      "[CV 2/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.639, test=0.651) total time=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END linearsvc__C=10, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.635, test=0.641) total time=   1.7s\n",
      "[CV 4/5] END linearsvc__C=10, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.621, test=0.608) total time=   1.8s\n",
      "[CV 3/5] END linearsvc__C=10, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.665, test=0.614) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END linearsvc__C=10, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.653, test=0.607) total time=   1.9s\n",
      "[CV 1/5] END linearsvc__C=10, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.648, test=0.616) total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END linearsvc__C=100, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.631, test=0.617) total time=   4.4s\n",
      "[CV 5/5] END linearsvc__C=100, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.641, test=0.593) total time=   4.5s\n",
      "[CV 4/5] END linearsvc__C=100, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.613, test=0.597) total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END linearsvc__C=100, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.651, test=0.615) total time=   4.6s\n",
      "[CV 2/5] END linearsvc__C=100, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.624, test=0.633) total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END linearsvc__C=1000, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.559, test=0.536) total time=   4.9s\n",
      "[CV 4/5] END linearsvc__C=1000, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.540, test=0.522) total time=   5.0s\n",
      "[CV 5/5] END linearsvc__C=1000, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.555, test=0.522) total time=   5.1s\n",
      "[CV 1/5] END linearsvc__C=1000, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.548, test=0.525) total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END linearsvc__C=1000, linearsvc__loss=hinge, linearsvc__penalty=l2;, score=(train=0.553, test=0.553) total time=   5.2s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 4\n",
      "n_resources: 13332\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV 3/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.642, test=0.615) total time=   0.4s\n",
      "[CV 2/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.636, test=0.617) total time=   0.4s\n",
      "[CV 4/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.643, test=0.622) total time=   0.4s\n",
      "[CV 1/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.633, test=0.624) total time=   0.4s\n",
      "[CV 5/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.634, test=0.628) total time=   0.4s\n",
      "[CV 1/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.633, test=0.624) total time=   1.3s\n",
      "[CV 3/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.642, test=0.615) total time=   1.3s\n",
      "[CV 5/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.634, test=0.628) total time=   1.4s\n",
      "[CV 5/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.634, test=0.628) total time=   1.4s\n",
      "[CV 4/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.643, test=0.622) total time=   1.4s\n",
      "[CV 5/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.634, test=0.628) total time=   1.5s\n",
      "[CV 3/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.642, test=0.615) total time=   1.5s\n",
      "[CV 2/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.636, test=0.617) total time=   1.5s\n",
      "[CV 4/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.643, test=0.622) total time=   1.5s\n",
      "[CV 3/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.642, test=0.615) total time=   1.5s\n",
      "[CV 4/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.643, test=0.622) total time=   1.5s\n",
      "[CV 2/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.636, test=0.617) total time=   1.5s\n",
      "[CV 1/5] END linearsvc__C=10, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.633, test=0.624) total time=   1.6s\n",
      "[CV 2/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.636, test=0.617) total time=   1.5s\n",
      "[CV 1/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.633, test=0.624) total time=   1.6s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 39996\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.632, test=0.633) total time=   0.6s\n",
      "[CV 3/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.634, test=0.625) total time=   0.6s\n",
      "[CV 2/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.633, test=0.627) total time=   0.7s\n",
      "[CV 5/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.632, test=0.634) total time=   0.7s\n",
      "[CV 4/5] END linearsvc__C=1, linearsvc__loss=squared_hinge, linearsvc__penalty=l2;, score=(train=0.633, test=0.626) total time=   0.7s\n",
      "[CV 3/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.634, test=0.625) total time=   2.2s\n",
      "[CV 5/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.632, test=0.634) total time=   2.3s\n",
      "[CV 4/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.633, test=0.626) total time=   2.4s\n",
      "[CV 1/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.632, test=0.633) total time=   2.4s\n",
      "[CV 2/5] END linearsvc__C=100, linearsvc__loss=squared_hinge, linearsvc__penalty=l1;, score=(train=0.633, test=0.627) total time=   2.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(estimator=Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                               StandardScaler()),\n",
       "                                              (&#x27;linearsvc&#x27;,\n",
       "                                               LinearSVC(dual=&#x27;auto&#x27;,\n",
       "                                                         max_iter=10000,\n",
       "                                                         random_state=1729))]),\n",
       "                    n_jobs=-1,\n",
       "                    param_grid=[{&#x27;linearsvc__C&#x27;: [1, 10, 100, 1000],\n",
       "                                 &#x27;linearsvc__loss&#x27;: [&#x27;squared_hinge&#x27;],\n",
       "                                 &#x27;linearsvc__penalty&#x27;: [&#x27;l1&#x27;]},\n",
       "                                {&#x27;linearsvc__C&#x27;: [1, 10, 100, 1000],\n",
       "                                 &#x27;linearsvc__loss&#x27;: [&#x27;hinge&#x27;, &#x27;squared_hinge&#x27;],\n",
       "                                 &#x27;linearsvc__penalty&#x27;: [&#x27;l2&#x27;]}],\n",
       "                    scoring=&#x27;roc_auc&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(estimator=Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                               StandardScaler()),\n",
       "                                              (&#x27;linearsvc&#x27;,\n",
       "                                               LinearSVC(dual=&#x27;auto&#x27;,\n",
       "                                                         max_iter=10000,\n",
       "                                                         random_state=1729))]),\n",
       "                    n_jobs=-1,\n",
       "                    param_grid=[{&#x27;linearsvc__C&#x27;: [1, 10, 100, 1000],\n",
       "                                 &#x27;linearsvc__loss&#x27;: [&#x27;squared_hinge&#x27;],\n",
       "                                 &#x27;linearsvc__penalty&#x27;: [&#x27;l1&#x27;]},\n",
       "                                {&#x27;linearsvc__C&#x27;: [1, 10, 100, 1000],\n",
       "                                 &#x27;linearsvc__loss&#x27;: [&#x27;hinge&#x27;, &#x27;squared_hinge&#x27;],\n",
       "                                 &#x27;linearsvc__penalty&#x27;: [&#x27;l2&#x27;]}],\n",
       "                    scoring=&#x27;roc_auc&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;linearsvc&#x27;,\n",
       "                 LinearSVC(dual=&#x27;auto&#x27;, max_iter=10000, random_state=1729))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=&#x27;auto&#x27;, max_iter=10000, random_state=1729)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(estimator=Pipeline(steps=[('standardscaler',\n",
       "                                               StandardScaler()),\n",
       "                                              ('linearsvc',\n",
       "                                               LinearSVC(dual='auto',\n",
       "                                                         max_iter=10000,\n",
       "                                                         random_state=1729))]),\n",
       "                    n_jobs=-1,\n",
       "                    param_grid=[{'linearsvc__C': [1, 10, 100, 1000],\n",
       "                                 'linearsvc__loss': ['squared_hinge'],\n",
       "                                 'linearsvc__penalty': ['l1']},\n",
       "                                {'linearsvc__C': [1, 10, 100, 1000],\n",
       "                                 'linearsvc__loss': ['hinge', 'squared_hinge'],\n",
       "                                 'linearsvc__penalty': ['l2']}],\n",
       "                    scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "linear_svc_params = [\n",
    "    {\n",
    "        'linearsvc__penalty': ['l1'],\n",
    "        'linearsvc__loss': ['squared_hinge'],\n",
    "        'linearsvc__C': [1,10,100,1000],\n",
    "    },\n",
    "    {\n",
    "        'linearsvc__penalty': ['l2'],\n",
    "        'linearsvc__loss': ['hinge', 'squared_hinge'],\n",
    "        'linearsvc__C': [1,10,100,1000],\n",
    "    },\n",
    "]\n",
    "\n",
    "linear_svc_grid = HalvingGridSearchCV(\n",
    "    make_pipeline(StandardScaler(), LinearSVC(dual='auto', random_state=1729, max_iter=10_000)),\n",
    "    linear_svc_params,\n",
    "    verbose=3,\n",
    "    scoring='roc_auc',\n",
    "    refit=True,\n",
    "    n_jobs=-1)\n",
    "linear_svc_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>param_linearsvc__C</th>\n",
       "      <th>param_linearsvc__loss</th>\n",
       "      <th>param_linearsvc__penalty</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.345440</td>\n",
       "      <td>0.008695</td>\n",
       "      <td>100</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.628981</td>\n",
       "      <td>0.003868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.654107</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>1</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.628981</td>\n",
       "      <td>0.003869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.404301</td>\n",
       "      <td>0.008978</td>\n",
       "      <td>1</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.621349</td>\n",
       "      <td>0.004704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.370390</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>100</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.621347</td>\n",
       "      <td>0.004727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.485988</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>1</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.621345</td>\n",
       "      <td>0.004707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.492851</td>\n",
       "      <td>0.004229</td>\n",
       "      <td>10</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.621345</td>\n",
       "      <td>0.004730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.598813</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>1</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.620152</td>\n",
       "      <td>0.016198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>1</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.620102</td>\n",
       "      <td>0.016148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.937091</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>10</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.620049</td>\n",
       "      <td>0.016049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.535929</td>\n",
       "      <td>0.007483</td>\n",
       "      <td>100</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.620044</td>\n",
       "      <td>0.016043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.093133</td>\n",
       "      <td>0.006195</td>\n",
       "      <td>100</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.620042</td>\n",
       "      <td>0.016038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.508191</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>1000</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.620037</td>\n",
       "      <td>0.016044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.091774</td>\n",
       "      <td>0.006312</td>\n",
       "      <td>1000</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.620037</td>\n",
       "      <td>0.016041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.077363</td>\n",
       "      <td>0.007056</td>\n",
       "      <td>10</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.620034</td>\n",
       "      <td>0.016040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.402060</td>\n",
       "      <td>0.006202</td>\n",
       "      <td>1</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.617650</td>\n",
       "      <td>0.012836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.816586</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>10</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.617327</td>\n",
       "      <td>0.012295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.545554</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>100</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.610948</td>\n",
       "      <td>0.014446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.046592</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>1000</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.531568</td>\n",
       "      <td>0.012028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time param_linearsvc__C param_linearsvc__loss  \\\n",
       "16       2.345440         0.008695                100         squared_hinge   \n",
       "17       0.654107         0.011000                  1         squared_hinge   \n",
       "14       0.404301         0.008978                  1         squared_hinge   \n",
       "12       1.370390         0.005886                100         squared_hinge   \n",
       "15       1.485988         0.004828                  1         squared_hinge   \n",
       "13       1.492851         0.004229                 10         squared_hinge   \n",
       "0        0.598813         0.005964                  1         squared_hinge   \n",
       "5        0.066084         0.006143                  1         squared_hinge   \n",
       "1        0.937091         0.005859                 10         squared_hinge   \n",
       "2        0.535929         0.007483                100         squared_hinge   \n",
       "9        0.093133         0.006195                100         squared_hinge   \n",
       "3        0.508191         0.007014               1000         squared_hinge   \n",
       "11       0.091774         0.006312               1000         squared_hinge   \n",
       "7        0.077363         0.007056                 10         squared_hinge   \n",
       "4        0.402060         0.006202                  1                 hinge   \n",
       "6        1.816586         0.003881                 10                 hinge   \n",
       "8        4.545554         0.003234                100                 hinge   \n",
       "10       5.046592         0.002476               1000                 hinge   \n",
       "\n",
       "   param_linearsvc__penalty  mean_test_score  std_test_score  \n",
       "16                       l1         0.628981        0.003868  \n",
       "17                       l2         0.628981        0.003869  \n",
       "14                       l2         0.621349        0.004704  \n",
       "12                       l1         0.621347        0.004727  \n",
       "15                       l1         0.621345        0.004707  \n",
       "13                       l1         0.621345        0.004730  \n",
       "0                        l1         0.620152        0.016198  \n",
       "5                        l2         0.620102        0.016148  \n",
       "1                        l1         0.620049        0.016049  \n",
       "2                        l1         0.620044        0.016043  \n",
       "9                        l2         0.620042        0.016038  \n",
       "3                        l1         0.620037        0.016044  \n",
       "11                       l2         0.620037        0.016041  \n",
       "7                        l2         0.620034        0.016040  \n",
       "4                        l2         0.617650        0.012836  \n",
       "6                        l2         0.617327        0.012295  \n",
       "8                        l2         0.610948        0.014446  \n",
       "10                       l2         0.531568        0.012028  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_grid_df = pd.DataFrame(linear_svc_grid.cv_results_)\n",
    "linear_grid_df.sort_values('mean_test_score', ascending=False)[['mean_fit_time', 'mean_score_time', 'param_linearsvc__C', 'param_linearsvc__loss', 'param_linearsvc__penalty', 'mean_test_score', 'std_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5739375497181516, 0.6108)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_linear_svc = linear_svc_grid.predict(X_test)\n",
    "(roc_auc_score(y_test, y_test_linear_svc), accuracy_score(y_test, y_test_linear_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos - separa√ß√£o complexa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plug-in\n",
    "\n",
    "K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 4444\n",
      "max_resources_: 40000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 24\n",
      "n_resources: 4444\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV 1/5] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.843, test=0.546) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.838, test=0.531) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.840, test=0.523) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.839, test=0.531) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.847, test=0.542) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.542) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.540) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.526) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.534) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.547) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.786, test=0.542) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.785, test=0.520) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.781, test=0.543) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.775, test=0.548) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.787, test=0.571) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.540) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.529) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.542) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.550) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.573) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=euclidean, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.720, test=0.560) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=euclidean, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.726, test=0.521) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=euclidean, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.718, test=0.549) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=euclidean, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.716, test=0.564) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=euclidean, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.726, test=0.564) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=euclidean, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.560) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=euclidean, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.522) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=euclidean, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.550) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=euclidean, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.566) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=euclidean, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.572) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=euclidean, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.678, test=0.595) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=euclidean, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.680, test=0.533) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=euclidean, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.663, test=0.549) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=euclidean, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.671, test=0.560) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=euclidean, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.689, test=0.567) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=euclidean, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=euclidean, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.533) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=euclidean, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.552) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=euclidean, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.561) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=euclidean, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.571) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.852, test=0.563) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.851, test=0.564) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.851, test=0.551) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.853, test=0.560) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.853, test=0.539) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.572) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.565) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.552) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.568) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.557) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.788, test=0.565) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.790, test=0.548) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.788, test=0.559) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.788, test=0.555) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.793, test=0.561) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.572) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.555) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.565) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.564) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.569) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.727, test=0.579) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.729, test=0.547) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.728, test=0.552) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.722, test=0.557) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.739, test=0.587) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.586) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.550) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.557) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.565) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.593) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.683, test=0.586) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.675, test=0.555) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.678, test=0.569) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.677, test=0.557) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.702, test=0.597) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.591) total time=   0.0s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.556) total time=   0.0s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.572) total time=   0.0s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.562) total time=   0.0s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.602) total time=   0.0s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.864, test=0.559) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.871, test=0.542) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.856, test=0.542) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.857, test=0.556) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=uniform;, score=(train=0.863, test=0.585) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.593) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.563) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.570) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.573) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.615) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.796, test=0.542) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.810, test=0.522) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.793, test=0.539) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.797, test=0.552) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=uniform;, score=(train=0.807, test=0.591) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.574) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.545) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.562) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.572) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.619) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.719, test=0.530) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.731, test=0.519) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.717, test=0.538) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.716, test=0.532) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.735, test=0.569) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.550) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.531) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.551) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.546) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.591) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.670, test=0.557) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.662, test=0.516) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.664, test=0.559) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.658, test=0.549) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.675, test=0.567) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.568) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.522) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.566) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.558) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.580) total time=   0.1s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 8\n",
      "n_resources: 13332\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.647) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.600) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.615) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.634) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.629) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.756, test=0.633) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.760, test=0.595) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.756, test=0.613) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.751, test=0.611) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=uniform;, score=(train=0.759, test=0.639) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.654) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.600) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.625) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.636) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.646) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.651) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.605) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.624) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.627) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=11, knn__weights=distance;, score=(train=1.000, test=0.652) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.709, test=0.638) total time=   0.1s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.706, test=0.597) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.703, test=0.620) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.703, test=0.611) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=uniform;, score=(train=0.708, test=0.629) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.711) total time=   0.6s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.678) total time=   0.6s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.687) total time=   0.6s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.697) total time=   0.6s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.699) total time=   0.6s\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.652) total time=   0.2s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.607) total time=   0.1s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.630) total time=   0.1s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.623) total time=   0.1s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=25, knn__weights=distance;, score=(train=1.000, test=0.642) total time=   0.1s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.717) total time=   0.6s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.690) total time=   0.6s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.691) total time=   0.6s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.705) total time=   0.6s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.702) total time=   0.6s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 3\n",
      "n_resources: 39996\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.692) total time=   1.0s\n",
      "[CV 2/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.688) total time=   0.9s\n",
      "[CV 3/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.684) total time=   0.9s\n",
      "[CV 4/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.691) total time=   0.9s\n",
      "[CV 5/5] END knn__metric=manhattan, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.685) total time=   0.9s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.802) total time=   6.1s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.798) total time=   6.0s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.799) total time=   5.8s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.798) total time=   6.4s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=5, knn__weights=distance;, score=(train=1.000, test=0.795) total time=   5.9s\n",
      "[CV 1/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.773) total time=   6.6s\n",
      "[CV 2/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.766) total time=   5.6s\n",
      "[CV 3/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.772) total time=   6.1s\n",
      "[CV 4/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.766) total time=   5.9s\n",
      "[CV 5/5] END knn__metric=hamming, knn__n_neighbors=3, knn__weights=distance;, score=(train=1.000, test=0.761) total time=   5.9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                              (&#x27;knn&#x27;, KNeighborsClassifier())]),\n",
       "                    n_jobs=1,\n",
       "                    param_grid=[{&#x27;knn__metric&#x27;: [&#x27;euclidean&#x27;, &#x27;manhattan&#x27;,\n",
       "                                                 &#x27;hamming&#x27;],\n",
       "                                 &#x27;knn__n_neighbors&#x27;: [3, 5, 11, 25],\n",
       "                                 &#x27;knn__weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]}],\n",
       "                    scoring=&#x27;roc_auc&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                              (&#x27;knn&#x27;, KNeighborsClassifier())]),\n",
       "                    n_jobs=1,\n",
       "                    param_grid=[{&#x27;knn__metric&#x27;: [&#x27;euclidean&#x27;, &#x27;manhattan&#x27;,\n",
       "                                                 &#x27;hamming&#x27;],\n",
       "                                 &#x27;knn__n_neighbors&#x27;: [3, 5, 11, 25],\n",
       "                                 &#x27;knn__weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]}],\n",
       "                    scoring=&#x27;roc_auc&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;knn&#x27;, KNeighborsClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                              ('knn', KNeighborsClassifier())]),\n",
       "                    n_jobs=1,\n",
       "                    param_grid=[{'knn__metric': ['euclidean', 'manhattan',\n",
       "                                                 'hamming'],\n",
       "                                 'knn__n_neighbors': [3, 5, 11, 25],\n",
       "                                 'knn__weights': ['uniform', 'distance']}],\n",
       "                    scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_params = [\n",
    "    {\n",
    "        'knn__n_neighbors': [3,5,11,25],\n",
    "        'knn__weights': ['uniform', 'distance'],\n",
    "        'knn__metric': ['euclidean', 'manhattan', 'hamming'],\n",
    "    },\n",
    "]\n",
    "\n",
    "knn_grid = HalvingGridSearchCV(\n",
    "    Pipeline(steps=[('scaler', StandardScaler()), ('knn', KNeighborsClassifier())]),\n",
    "    knn_params,\n",
    "    verbose=3,\n",
    "    scoring='roc_auc',\n",
    "    refit=True,\n",
    "    n_jobs=1)\n",
    "knn_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>param_knn__metric</th>\n",
       "      <th>param_knn__n_neighbors</th>\n",
       "      <th>param_knn__weights</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.036875</td>\n",
       "      <td>5.995157</td>\n",
       "      <td>hamming</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.798293</td>\n",
       "      <td>0.002305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.036986</td>\n",
       "      <td>5.985223</td>\n",
       "      <td>hamming</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.767478</td>\n",
       "      <td>0.004488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.012103</td>\n",
       "      <td>0.562652</td>\n",
       "      <td>hamming</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.700772</td>\n",
       "      <td>0.009877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.011589</td>\n",
       "      <td>0.584453</td>\n",
       "      <td>hamming</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.694450</td>\n",
       "      <td>0.011259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.038471</td>\n",
       "      <td>0.895380</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.688167</td>\n",
       "      <td>0.003380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.011861</td>\n",
       "      <td>0.114986</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.632087</td>\n",
       "      <td>0.019062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.011892</td>\n",
       "      <td>0.122411</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.631938</td>\n",
       "      <td>0.017685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.011683</td>\n",
       "      <td>0.134732</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>25</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.630629</td>\n",
       "      <td>0.015598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.011467</td>\n",
       "      <td>0.119229</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.624885</td>\n",
       "      <td>0.016070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.011736</td>\n",
       "      <td>0.128731</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>25</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.619147</td>\n",
       "      <td>0.014128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.012040</td>\n",
       "      <td>0.122028</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.618295</td>\n",
       "      <td>0.015662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005457</td>\n",
       "      <td>0.060204</td>\n",
       "      <td>hamming</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.582751</td>\n",
       "      <td>0.018725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.005838</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>25</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.576702</td>\n",
       "      <td>0.017511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005783</td>\n",
       "      <td>0.064849</td>\n",
       "      <td>hamming</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.574292</td>\n",
       "      <td>0.024317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005978</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>25</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.572675</td>\n",
       "      <td>0.016187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.570237</td>\n",
       "      <td>0.016677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.005636</td>\n",
       "      <td>0.019147</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.565092</td>\n",
       "      <td>0.005711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006178</td>\n",
       "      <td>0.019347</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.564542</td>\n",
       "      <td>0.015430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.006232</td>\n",
       "      <td>0.018410</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.562695</td>\n",
       "      <td>0.007093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005354</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>25</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.562043</td>\n",
       "      <td>0.020379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.006391</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>25</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.560814</td>\n",
       "      <td>0.020685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.063413</td>\n",
       "      <td>hamming</td>\n",
       "      <td>25</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.558843</td>\n",
       "      <td>0.019660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005892</td>\n",
       "      <td>0.019786</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.557546</td>\n",
       "      <td>0.005806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005751</td>\n",
       "      <td>0.059947</td>\n",
       "      <td>hamming</td>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.556623</td>\n",
       "      <td>0.015839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.006053</td>\n",
       "      <td>0.018823</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.555304</td>\n",
       "      <td>0.009390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006456</td>\n",
       "      <td>0.008529</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.554132</td>\n",
       "      <td>0.017649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005820</td>\n",
       "      <td>0.065528</td>\n",
       "      <td>hamming</td>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.553601</td>\n",
       "      <td>0.019813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005509</td>\n",
       "      <td>0.006492</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.551681</td>\n",
       "      <td>0.016428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.064993</td>\n",
       "      <td>hamming</td>\n",
       "      <td>25</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.549691</td>\n",
       "      <td>0.017978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.063124</td>\n",
       "      <td>hamming</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.549314</td>\n",
       "      <td>0.022867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005796</td>\n",
       "      <td>0.007472</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.546856</td>\n",
       "      <td>0.014726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006545</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.545057</td>\n",
       "      <td>0.016346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006271</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.537971</td>\n",
       "      <td>0.007124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.005501</td>\n",
       "      <td>0.062966</td>\n",
       "      <td>hamming</td>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.537696</td>\n",
       "      <td>0.016962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006905</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.534821</td>\n",
       "      <td>0.008438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time param_knn__metric param_knn__n_neighbors  \\\n",
       "33       0.036875         5.995157           hamming                      5   \n",
       "34       0.036986         5.985223           hamming                      3   \n",
       "31       0.012103         0.562652           hamming                      3   \n",
       "29       0.011589         0.584453           hamming                      5   \n",
       "32       0.038471         0.895380         manhattan                      5   \n",
       "26       0.011861         0.114986         manhattan                      5   \n",
       "27       0.011892         0.122411         manhattan                     11   \n",
       "30       0.011683         0.134732         manhattan                     25   \n",
       "24       0.011467         0.119229         manhattan                      3   \n",
       "28       0.011736         0.128731         manhattan                     25   \n",
       "25       0.012040         0.122028         manhattan                     11   \n",
       "17       0.005457         0.060204           hamming                      3   \n",
       "15       0.005838         0.022918         manhattan                     25   \n",
       "19       0.005783         0.064849           hamming                      5   \n",
       "14       0.005978         0.020862         manhattan                     25   \n",
       "13       0.006308         0.020761         manhattan                     11   \n",
       "11       0.005636         0.019147         manhattan                      5   \n",
       "12       0.006178         0.019347         manhattan                     11   \n",
       "9        0.006232         0.018410         manhattan                      3   \n",
       "7        0.005354         0.009896         euclidean                     25   \n",
       "6        0.006391         0.009438         euclidean                     25   \n",
       "23       0.005639         0.063413           hamming                     25   \n",
       "10       0.005892         0.019786         manhattan                      5   \n",
       "16       0.005751         0.059947           hamming                      3   \n",
       "8        0.006053         0.018823         manhattan                      3   \n",
       "5        0.006456         0.008529         euclidean                     11   \n",
       "21       0.005820         0.065528           hamming                     11   \n",
       "4        0.005509         0.006492         euclidean                     11   \n",
       "22       0.005379         0.064993           hamming                     25   \n",
       "18       0.005490         0.063124           hamming                      5   \n",
       "3        0.005796         0.007472         euclidean                      5   \n",
       "2        0.006545         0.005692         euclidean                      5   \n",
       "1        0.006271         0.006484         euclidean                      3   \n",
       "20       0.005501         0.062966           hamming                     11   \n",
       "0        0.006905         0.028935         euclidean                      3   \n",
       "\n",
       "   param_knn__weights  mean_test_score  std_test_score  \n",
       "33           distance         0.798293        0.002305  \n",
       "34           distance         0.767478        0.004488  \n",
       "31           distance         0.700772        0.009877  \n",
       "29           distance         0.694450        0.011259  \n",
       "32           distance         0.688167        0.003380  \n",
       "26           distance         0.632087        0.019062  \n",
       "27           distance         0.631938        0.017685  \n",
       "30           distance         0.630629        0.015598  \n",
       "24           distance         0.624885        0.016070  \n",
       "28            uniform         0.619147        0.014128  \n",
       "25            uniform         0.618295        0.015662  \n",
       "17           distance         0.582751        0.018725  \n",
       "15           distance         0.576702        0.017511  \n",
       "19           distance         0.574292        0.024317  \n",
       "14            uniform         0.572675        0.016187  \n",
       "13           distance         0.570237        0.016677  \n",
       "11           distance         0.565092        0.005711  \n",
       "12            uniform         0.564542        0.015430  \n",
       "9            distance         0.562695        0.007093  \n",
       "7            distance         0.562043        0.020379  \n",
       "6             uniform         0.560814        0.020685  \n",
       "23           distance         0.558843        0.019660  \n",
       "10            uniform         0.557546        0.005806  \n",
       "16            uniform         0.556623        0.015839  \n",
       "8             uniform         0.555304        0.009390  \n",
       "5            distance         0.554132        0.017649  \n",
       "21           distance         0.553601        0.019813  \n",
       "4             uniform         0.551681        0.016428  \n",
       "22            uniform         0.549691        0.017978  \n",
       "18            uniform         0.549314        0.022867  \n",
       "3            distance         0.546856        0.014726  \n",
       "2             uniform         0.545057        0.016346  \n",
       "1            distance         0.537971        0.007124  \n",
       "20            uniform         0.537696        0.016962  \n",
       "0             uniform         0.534821        0.008438  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid_df = pd.DataFrame(knn_grid.cv_results_)\n",
    "knn_grid_df.sort_values('mean_test_score', ascending=False)[['mean_fit_time', 'mean_score_time', 'param_knn__metric', 'param_knn__n_neighbors', 'param_knn__weights', 'mean_test_score', 'std_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7317498850541544, 0.7439)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_knn = knn_grid.predict(X_test)\n",
    "(roc_auc_score(y_test, y_test_knn), accuracy_score(y_test, y_test_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_grid.model']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(knn_grid, 'knn_grid.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk minimization\n",
    "\n",
    "Modelo de SVC com Kernel n√£o linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 1481\n",
      "max_resources_: 40000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 64\n",
      "n_resources: 1481\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.992, test=0.557) total time=   0.2s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.996, test=0.571) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.994, test=0.619) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.994, test=0.578) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.997, test=0.563) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.564) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.519) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.581) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.576) total time=   0.1s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.564) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.519) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.581) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.576) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.564) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.519) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.581) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.576) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.545) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.532) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.592) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.562) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.519) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.514) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.514) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.514) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.441, test=0.526) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.403, test=0.482) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.374, test=0.569) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.384, test=0.450) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.371, test=0.531) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.648, test=0.509) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.635, test=0.467) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.628, test=0.527) total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.619, test=0.519) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.633, test=0.554) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.554) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.521) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.608) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.569) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.580) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.520) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.610) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.575) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=-1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.578) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.446, test=0.521) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.425, test=0.482) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.391, test=0.543) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.409, test=0.447) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.391, test=0.544) total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.738, test=0.519) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.739, test=0.491) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.739, test=0.557) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.747, test=0.469) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.726, test=0.556) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.513) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.593) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.545) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.607) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.513) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.545) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=-1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.980, test=0.562) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.982, test=0.559) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.989, test=0.625) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.979, test=0.628) total time=   0.1s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.986, test=0.573) total time=   0.1s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.550) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.535) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.613) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.553) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.568) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.521) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.610) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.587) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.570) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.565) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.521) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.610) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.584) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.572) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.558) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.553) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.615) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.590) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.554) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.553) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.531) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.572) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.580) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.520) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.592) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.554) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.603) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.547) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.517) total time=   0.1s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.552) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.607) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.582) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.540) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.626) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.593) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.593) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.564) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.519) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.581) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.576) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.564) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.519) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.581) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.576) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.564) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.519) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.581) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.576) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.551) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.540) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.596) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.593) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.581) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.514) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.514) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.514) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.442, test=0.521) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.398, test=0.481) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.371, test=0.548) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.401, test=0.444) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.371, test=0.555) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.646, test=0.527) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.636, test=0.483) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.635, test=0.490) total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.634, test=0.492) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.633, test=0.534) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.554) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.521) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.608) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.569) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.580) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.520) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.610) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.575) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=-1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.578) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.451, test=0.540) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.415, test=0.451) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.400, test=0.537) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.419, test=0.459) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.394, test=0.541) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.739, test=0.521) total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.732, test=0.488) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.740, test=0.565) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.731, test=0.470) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=0.726, test=0.557) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.513) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.593) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.545) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.607) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.513) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.545) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=-1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.609) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.537) total time=   0.1s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.541) total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.631) total time=   0.1s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.591) total time=   0.1s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.552) total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.550) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.535) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.613) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.553) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.568) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.521) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.610) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.587) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.570) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.565) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.521) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.610) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.584) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.572) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.615) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.587) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.549) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.553) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.531) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.572) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.580) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.520) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.592) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.554) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.603) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.547) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.517) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.594) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.552) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.607) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.950, test=0.577) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.951, test=0.568) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.958, test=0.638) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.944, test=0.588) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.962, test=0.572) total time=   0.0s\n",
      "[CV 1/5] END svc__C=1, svc__gamma=scale, svc__kernel=sigmoid;, score=(train=0.510, test=0.585) total time=   0.1s\n",
      "[CV 2/5] END svc__C=1, svc__gamma=scale, svc__kernel=sigmoid;, score=(train=0.422, test=0.525) total time=   0.1s\n",
      "[CV 3/5] END svc__C=1, svc__gamma=scale, svc__kernel=sigmoid;, score=(train=0.410, test=0.634) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__gamma=scale, svc__kernel=sigmoid;, score=(train=0.402, test=0.534) total time=   0.1s\n",
      "[CV 5/5] END svc__C=1, svc__gamma=scale, svc__kernel=sigmoid;, score=(train=0.353, test=0.549) total time=   0.1s\n",
      "[CV 1/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.575) total time=   0.1s\n",
      "[CV 2/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.553) total time=   0.1s\n",
      "[CV 3/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.597) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.565) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.582) total time=   0.1s\n",
      "[CV 1/5] END svc__C=1, svc__gamma=0.1, svc__kernel=sigmoid;, score=(train=0.340, test=0.519) total time=   0.1s\n",
      "[CV 2/5] END svc__C=1, svc__gamma=0.1, svc__kernel=sigmoid;, score=(train=0.346, test=0.495) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__gamma=0.1, svc__kernel=sigmoid;, score=(train=0.327, test=0.555) total time=   0.1s\n",
      "[CV 4/5] END svc__C=1, svc__gamma=0.1, svc__kernel=sigmoid;, score=(train=0.340, test=0.493) total time=   0.1s\n",
      "[CV 5/5] END svc__C=1, svc__gamma=0.1, svc__kernel=sigmoid;, score=(train=0.301, test=0.479) total time=   0.1s\n",
      "[CV 1/5] END svc__C=1, svc__gamma=1, svc__kernel=rbf;, score=(train=1.000, test=0.573) total time=   0.1s\n",
      "[CV 2/5] END svc__C=1, svc__gamma=1, svc__kernel=rbf;, score=(train=1.000, test=0.494) total time=   0.1s\n",
      "[CV 3/5] END svc__C=1, svc__gamma=1, svc__kernel=rbf;, score=(train=1.000, test=0.516) total time=   0.1s\n",
      "[CV 4/5] END svc__C=1, svc__gamma=1, svc__kernel=rbf;, score=(train=1.000, test=0.537) total time=   0.1s\n",
      "[CV 5/5] END svc__C=1, svc__gamma=1, svc__kernel=rbf;, score=(train=1.000, test=0.567) total time=   0.1s\n",
      "[CV 1/5] END svc__C=1, svc__gamma=1, svc__kernel=sigmoid;, score=(train=0.359, test=0.493) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__gamma=1, svc__kernel=sigmoid;, score=(train=0.359, test=0.512) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__gamma=1, svc__kernel=sigmoid;, score=(train=0.355, test=0.508) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__gamma=1, svc__kernel=sigmoid;, score=(train=0.336, test=0.531) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__gamma=1, svc__kernel=sigmoid;, score=(train=0.328, test=0.458) total time=   0.1s\n",
      "[CV 1/5] END svc__C=1, svc__gamma=2, svc__kernel=rbf;, score=(train=1.000, test=0.508) total time=   0.1s\n",
      "[CV 2/5] END svc__C=1, svc__gamma=2, svc__kernel=rbf;, score=(train=1.000, test=0.501) total time=   0.1s\n",
      "[CV 3/5] END svc__C=1, svc__gamma=2, svc__kernel=rbf;, score=(train=1.000, test=0.497) total time=   0.1s\n",
      "[CV 4/5] END svc__C=1, svc__gamma=2, svc__kernel=rbf;, score=(train=1.000, test=0.526) total time=   0.1s\n",
      "[CV 5/5] END svc__C=1, svc__gamma=2, svc__kernel=rbf;, score=(train=1.000, test=0.517) total time=   0.1s\n",
      "[CV 1/5] END svc__C=1, svc__gamma=2, svc__kernel=sigmoid;, score=(train=0.339, test=0.470) total time=   0.0s\n",
      "[CV 2/5] END svc__C=1, svc__gamma=2, svc__kernel=sigmoid;, score=(train=0.364, test=0.498) total time=   0.0s\n",
      "[CV 3/5] END svc__C=1, svc__gamma=2, svc__kernel=sigmoid;, score=(train=0.362, test=0.500) total time=   0.0s\n",
      "[CV 4/5] END svc__C=1, svc__gamma=2, svc__kernel=sigmoid;, score=(train=0.347, test=0.526) total time=   0.0s\n",
      "[CV 5/5] END svc__C=1, svc__gamma=2, svc__kernel=sigmoid;, score=(train=0.322, test=0.483) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=1.000, test=0.564) total time=   0.1s\n",
      "[CV 2/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=1.000, test=0.583) total time=   0.1s\n",
      "[CV 3/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=1.000, test=0.638) total time=   0.1s\n",
      "[CV 4/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=1.000, test=0.591) total time=   0.1s\n",
      "[CV 5/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=1.000, test=0.565) total time=   0.1s\n",
      "[CV 1/5] END svc__C=10, svc__gamma=scale, svc__kernel=sigmoid;, score=(train=0.344, test=0.537) total time=   0.1s\n",
      "[CV 2/5] END svc__C=10, svc__gamma=scale, svc__kernel=sigmoid;, score=(train=0.317, test=0.478) total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__gamma=scale, svc__kernel=sigmoid;, score=(train=0.285, test=0.603) total time=   0.1s\n",
      "[CV 4/5] END svc__C=10, svc__gamma=scale, svc__kernel=sigmoid;, score=(train=0.313, test=0.456) total time=   0.1s\n",
      "[CV 5/5] END svc__C=10, svc__gamma=scale, svc__kernel=sigmoid;, score=(train=0.273, test=0.531) total time=   0.1s\n",
      "[CV 1/5] END svc__C=10, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.569) total time=   0.1s\n",
      "[CV 2/5] END svc__C=10, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.547) total time=   0.1s\n",
      "[CV 3/5] END svc__C=10, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.592) total time=   0.1s\n",
      "[CV 4/5] END svc__C=10, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.554) total time=   0.1s\n",
      "[CV 5/5] END svc__C=10, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.575) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__gamma=0.1, svc__kernel=sigmoid;, score=(train=0.352, test=0.506) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__gamma=0.1, svc__kernel=sigmoid;, score=(train=0.327, test=0.507) total time=   0.1s\n",
      "[CV 3/5] END svc__C=10, svc__gamma=0.1, svc__kernel=sigmoid;, score=(train=0.320, test=0.572) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__gamma=0.1, svc__kernel=sigmoid;, score=(train=0.342, test=0.492) total time=   0.1s\n",
      "[CV 5/5] END svc__C=10, svc__gamma=0.1, svc__kernel=sigmoid;, score=(train=0.304, test=0.506) total time=   0.1s\n",
      "[CV 1/5] END svc__C=10, svc__gamma=1, svc__kernel=rbf;, score=(train=1.000, test=0.572) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__gamma=1, svc__kernel=rbf;, score=(train=1.000, test=0.492) total time=   0.1s\n",
      "[CV 3/5] END svc__C=10, svc__gamma=1, svc__kernel=rbf;, score=(train=1.000, test=0.516) total time=   0.1s\n",
      "[CV 4/5] END svc__C=10, svc__gamma=1, svc__kernel=rbf;, score=(train=1.000, test=0.536) total time=   0.1s\n",
      "[CV 5/5] END svc__C=10, svc__gamma=1, svc__kernel=rbf;, score=(train=1.000, test=0.569) total time=   0.1s\n",
      "[CV 1/5] END svc__C=10, svc__gamma=1, svc__kernel=sigmoid;, score=(train=0.348, test=0.508) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__gamma=1, svc__kernel=sigmoid;, score=(train=0.356, test=0.519) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__gamma=1, svc__kernel=sigmoid;, score=(train=0.352, test=0.469) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__gamma=1, svc__kernel=sigmoid;, score=(train=0.337, test=0.530) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__gamma=1, svc__kernel=sigmoid;, score=(train=0.323, test=0.478) total time=   0.0s\n",
      "[CV 1/5] END svc__C=10, svc__gamma=2, svc__kernel=rbf;, score=(train=1.000, test=0.508) total time=   0.1s\n",
      "[CV 2/5] END svc__C=10, svc__gamma=2, svc__kernel=rbf;, score=(train=1.000, test=0.501) total time=   0.1s\n",
      "[CV 3/5] END svc__C=10, svc__gamma=2, svc__kernel=rbf;, score=(train=1.000, test=0.495) total time=   0.1s\n",
      "[CV 4/5] END svc__C=10, svc__gamma=2, svc__kernel=rbf;, score=(train=1.000, test=0.523) total time=   0.1s\n",
      "[CV 5/5] END svc__C=10, svc__gamma=2, svc__kernel=rbf;, score=(train=1.000, test=0.523) total time=   0.1s\n",
      "[CV 1/5] END svc__C=10, svc__gamma=2, svc__kernel=sigmoid;, score=(train=0.348, test=0.474) total time=   0.0s\n",
      "[CV 2/5] END svc__C=10, svc__gamma=2, svc__kernel=sigmoid;, score=(train=0.359, test=0.495) total time=   0.0s\n",
      "[CV 3/5] END svc__C=10, svc__gamma=2, svc__kernel=sigmoid;, score=(train=0.365, test=0.495) total time=   0.0s\n",
      "[CV 4/5] END svc__C=10, svc__gamma=2, svc__kernel=sigmoid;, score=(train=0.339, test=0.539) total time=   0.0s\n",
      "[CV 5/5] END svc__C=10, svc__gamma=2, svc__kernel=sigmoid;, score=(train=0.324, test=0.477) total time=   0.0s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 22\n",
      "n_resources: 4443\n",
      "Fitting 5 folds for each of 22 candidates, totalling 110 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.564) total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.569) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.576) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.544) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.591) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.567) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.566) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.566) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.538) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.582) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.567) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.566) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.566) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.538) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.582) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.546) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.540) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.584) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.558) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.546) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.540) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=0.1, svc__kernel=poly;, score=(train=1.000, test=0.584) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.546) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.540) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.584) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.546) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.540) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.584) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.546) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.540) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.584) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.546) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.559) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.540) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.584) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.996, test=0.576) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.998, test=0.583) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.996, test=0.583) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.997, test=0.562) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.997, test=0.584) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.561) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.561) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.539) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.585) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.561) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.548) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.561) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.539) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=2, svc__kernel=poly;, score=(train=1.000, test=0.585) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.563) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.550) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.563) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.539) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.586) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.563) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.550) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.563) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.539) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=1, svc__kernel=poly;, score=(train=1.000, test=0.586) total time=   0.3s\n",
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.569) total time=   0.3s\n",
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.545) total time=   0.3s\n",
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.575) total time=   0.3s\n",
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.563) total time=   0.3s\n",
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.606) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.574) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.573) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.581) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.547) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.599) total time=   0.3s\n",
      "[CV 1/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.587) total time=   0.4s\n",
      "[CV 2/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.580) total time=   0.4s\n",
      "[CV 3/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.600) total time=   0.4s\n",
      "[CV 4/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.545) total time=   0.4s\n",
      "[CV 5/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.608) total time=   0.5s\n",
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.975, test=0.573) total time=   0.3s\n",
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.968, test=0.549) total time=   0.3s\n",
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.971, test=0.571) total time=   0.3s\n",
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.967, test=0.564) total time=   0.3s\n",
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.972, test=0.606) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.575) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.551) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.569) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.998, test=0.550) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=1.000, test=0.602) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.996, test=0.586) total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.997, test=0.574) total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.995, test=0.612) total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.996, test=0.559) total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.998, test=0.610) total time=   0.4s\n",
      "[CV 1/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.921, test=0.596) total time=   0.4s\n",
      "[CV 2/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.903, test=0.574) total time=   0.4s\n",
      "[CV 3/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.911, test=0.625) total time=   0.4s\n",
      "[CV 4/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.898, test=0.593) total time=   0.4s\n",
      "[CV 5/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.905, test=0.630) total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.944, test=0.588) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.940, test=0.566) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.947, test=0.612) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.945, test=0.575) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.946, test=0.608) total time=   0.3s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 8\n",
      "n_resources: 13329\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.604) total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.574) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.597) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.589) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=0, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.999, test=0.596) total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.917, test=0.613) total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.919, test=0.580) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.917, test=0.612) total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.917, test=0.593) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=0, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.917, test=0.596) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.982, test=0.603) total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.976, test=0.567) total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.977, test=0.585) total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.976, test=0.593) total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=5, svc__gamma=scale, svc__kernel=poly;, score=(train=0.980, test=0.584) total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.751, test=0.567) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.784, test=0.542) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.785, test=0.557) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.776, test=0.572) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.771, test=0.544) total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.616) total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.585) total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.607) total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.599) total time=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__gamma=0.1, svc__kernel=rbf;, score=(train=1.000, test=0.607) total time=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.980, test=0.631) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.982, test=0.593) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.981, test=0.616) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.984, test=0.606) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.980, test=0.617) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.859, test=0.629) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.860, test=0.596) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.866, test=0.623) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.859, test=0.609) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.857, test=0.623) total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.869, test=0.634) total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.869, test=0.598) total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.862, test=0.636) total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.870, test=0.631) total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.866, test=0.631) total time=   3.8s\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 3\n",
      "n_resources: 39987\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.762, test=0.584) total time=  17.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.792, test=0.584) total time=  15.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.776, test=0.572) total time=  15.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.783, test=0.593) total time=  15.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=10, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.781, test=0.576) total time=  15.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.626, test=0.566) total time=  12.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.617, test=0.556) total time=  13.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.626, test=0.564) total time=  12.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.629, test=0.563) total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__coef0=1, svc__degree=3, svc__gamma=scale, svc__kernel=poly;, score=(train=0.636, test=0.560) total time=  13.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.767, test=0.637) total time=  23.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.781, test=0.633) total time=  23.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.767, test=0.636) total time=  24.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.778, test=0.635) total time=  24.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END svc__C=1, svc__gamma=scale, svc__kernel=rbf;, score=(train=0.777, test=0.631) total time=  23.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guigb/.local/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                              (&#x27;svc&#x27;,\n",
       "                                               SVC(cache_size=2000,\n",
       "                                                   max_iter=10000,\n",
       "                                                   random_state=1729))]),\n",
       "                    n_jobs=1,\n",
       "                    param_grid=[{&#x27;svc__C&#x27;: [1, 10], &#x27;svc__coef0&#x27;: [0, -1, 1],\n",
       "                                 &#x27;svc__degree&#x27;: [3, 5],\n",
       "                                 &#x27;svc__gamma&#x27;: [&#x27;scale&#x27;, 0.1, 1, 2],\n",
       "                                 &#x27;svc__kernel&#x27;: [&#x27;poly&#x27;]},\n",
       "                                {&#x27;svc__C&#x27;: [1, 10],\n",
       "                                 &#x27;svc__gamma&#x27;: [&#x27;scale&#x27;, 0.1, 1, 2],\n",
       "                                 &#x27;svc__kernel&#x27;: [&#x27;rbf&#x27;, &#x27;sigmoid&#x27;]}],\n",
       "                    refit=&#x27;roc_auc&#x27;, scoring=&#x27;roc_auc&#x27;, verbose=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                              (&#x27;svc&#x27;,\n",
       "                                               SVC(cache_size=2000,\n",
       "                                                   max_iter=10000,\n",
       "                                                   random_state=1729))]),\n",
       "                    n_jobs=1,\n",
       "                    param_grid=[{&#x27;svc__C&#x27;: [1, 10], &#x27;svc__coef0&#x27;: [0, -1, 1],\n",
       "                                 &#x27;svc__degree&#x27;: [3, 5],\n",
       "                                 &#x27;svc__gamma&#x27;: [&#x27;scale&#x27;, 0.1, 1, 2],\n",
       "                                 &#x27;svc__kernel&#x27;: [&#x27;poly&#x27;]},\n",
       "                                {&#x27;svc__C&#x27;: [1, 10],\n",
       "                                 &#x27;svc__gamma&#x27;: [&#x27;scale&#x27;, 0.1, 1, 2],\n",
       "                                 &#x27;svc__kernel&#x27;: [&#x27;rbf&#x27;, &#x27;sigmoid&#x27;]}],\n",
       "                    refit=&#x27;roc_auc&#x27;, scoring=&#x27;roc_auc&#x27;, verbose=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svc&#x27;,\n",
       "                 SVC(cache_size=2000, max_iter=10000, random_state=1729))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(cache_size=2000, max_iter=10000, random_state=1729)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                              ('svc',\n",
       "                                               SVC(cache_size=2000,\n",
       "                                                   max_iter=10000,\n",
       "                                                   random_state=1729))]),\n",
       "                    n_jobs=1,\n",
       "                    param_grid=[{'svc__C': [1, 10], 'svc__coef0': [0, -1, 1],\n",
       "                                 'svc__degree': [3, 5],\n",
       "                                 'svc__gamma': ['scale', 0.1, 1, 2],\n",
       "                                 'svc__kernel': ['poly']},\n",
       "                                {'svc__C': [1, 10],\n",
       "                                 'svc__gamma': ['scale', 0.1, 1, 2],\n",
       "                                 'svc__kernel': ['rbf', 'sigmoid']}],\n",
       "                    refit='roc_auc', scoring='roc_auc', verbose=4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_params = [\n",
    "    {\n",
    "        'svc__C': [1, 10],\n",
    "        'svc__kernel': ['poly'],\n",
    "        'svc__degree': [3, 5],\n",
    "        'svc__gamma': ['scale', 0.1, 1, 2],\n",
    "        'svc__coef0': [0, -1, 1],\n",
    "    },\n",
    "    {\n",
    "        'svc__C': [1, 10],\n",
    "        'svc__kernel': ['rbf', 'sigmoid'],\n",
    "        'svc__gamma': ['scale', 0.1, 1, 2],\n",
    "    },\n",
    "]\n",
    "\n",
    "svc_grid = HalvingGridSearchCV(\n",
    "    Pipeline(steps=[('scaler',StandardScaler()), ('svc', SVC(random_state=1729, max_iter=10_000, cache_size=2000))]),\n",
    "    svc_params,\n",
    "    verbose=3,\n",
    "    scoring='roc_auc',\n",
    "    refit=True,\n",
    "    n_jobs=1)\n",
    "svc_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>param_svc__C</th>\n",
       "      <th>param_svc__kernel</th>\n",
       "      <th>param_svc__degree</th>\n",
       "      <th>param_svc__gamma</th>\n",
       "      <th>param_svc__coef0</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18.292804</td>\n",
       "      <td>5.607623</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.634268</td>\n",
       "      <td>0.002076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3.065523</td>\n",
       "      <td>0.809643</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.626097</td>\n",
       "      <td>0.014080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2.118250</td>\n",
       "      <td>0.294197</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.616236</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2.211192</td>\n",
       "      <td>0.769076</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.612662</td>\n",
       "      <td>0.012683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.287028</td>\n",
       "      <td>0.097163</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.603588</td>\n",
       "      <td>0.021176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>3.199683</td>\n",
       "      <td>0.966518</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.602853</td>\n",
       "      <td>0.010565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2.716950</td>\n",
       "      <td>0.327062</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>0</td>\n",
       "      <td>0.598732</td>\n",
       "      <td>0.012288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2.263081</td>\n",
       "      <td>0.377658</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>0</td>\n",
       "      <td>0.592111</td>\n",
       "      <td>0.010182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.284258</td>\n",
       "      <td>0.036106</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.589607</td>\n",
       "      <td>0.017931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.042063</td>\n",
       "      <td>0.007360</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.589307</td>\n",
       "      <td>0.030650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.030972</td>\n",
       "      <td>0.013612</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.588685</td>\n",
       "      <td>0.025528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.045521</td>\n",
       "      <td>0.013454</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.588274</td>\n",
       "      <td>0.027045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.325985</td>\n",
       "      <td>0.095508</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.588262</td>\n",
       "      <td>0.020385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.034146</td>\n",
       "      <td>0.006446</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>0</td>\n",
       "      <td>0.586753</td>\n",
       "      <td>0.027863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.752507</td>\n",
       "      <td>0.293310</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.586141</td>\n",
       "      <td>0.011901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.329916</td>\n",
       "      <td>0.111168</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.584069</td>\n",
       "      <td>0.021823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>11.988793</td>\n",
       "      <td>3.884694</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.581595</td>\n",
       "      <td>0.007319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.010551</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>0</td>\n",
       "      <td>0.577658</td>\n",
       "      <td>0.021641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.272696</td>\n",
       "      <td>0.034773</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.577536</td>\n",
       "      <td>0.008328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.272144</td>\n",
       "      <td>0.039011</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.574795</td>\n",
       "      <td>0.016676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.034453</td>\n",
       "      <td>0.015155</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.574287</td>\n",
       "      <td>0.015040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.040192</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573865</td>\n",
       "      <td>0.024639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.268680</td>\n",
       "      <td>0.040916</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>0</td>\n",
       "      <td>0.572715</td>\n",
       "      <td>0.018665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.033676</td>\n",
       "      <td>0.007318</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>0</td>\n",
       "      <td>0.572277</td>\n",
       "      <td>0.022845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.293910</td>\n",
       "      <td>0.047887</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571733</td>\n",
       "      <td>0.019988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.035482</td>\n",
       "      <td>0.007975</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.571318</td>\n",
       "      <td>0.029297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.034725</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.571318</td>\n",
       "      <td>0.029297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.034350</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.570531</td>\n",
       "      <td>0.029023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.035111</td>\n",
       "      <td>0.006443</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.570531</td>\n",
       "      <td>0.029023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.048804</td>\n",
       "      <td>0.006199</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.570379</td>\n",
       "      <td>0.035869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.035159</td>\n",
       "      <td>0.006589</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.569835</td>\n",
       "      <td>0.029430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.035051</td>\n",
       "      <td>0.006629</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.569835</td>\n",
       "      <td>0.029430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037916</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.569835</td>\n",
       "      <td>0.029430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.034064</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.569835</td>\n",
       "      <td>0.029430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.035377</td>\n",
       "      <td>0.007003</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.569816</td>\n",
       "      <td>0.029425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038150</td>\n",
       "      <td>0.008470</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.569816</td>\n",
       "      <td>0.029425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>0</td>\n",
       "      <td>0.569570</td>\n",
       "      <td>0.019304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.362969</td>\n",
       "      <td>0.038547</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.568876</td>\n",
       "      <td>0.015345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.037864</td>\n",
       "      <td>0.006231</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.568844</td>\n",
       "      <td>0.029519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.568844</td>\n",
       "      <td>0.029519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.040367</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.568722</td>\n",
       "      <td>0.027811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.034836</td>\n",
       "      <td>0.007184</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.568341</td>\n",
       "      <td>0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.034049</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.568341</td>\n",
       "      <td>0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.035467</td>\n",
       "      <td>0.015263</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.567325</td>\n",
       "      <td>0.015858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.034170</td>\n",
       "      <td>0.006691</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.566279</td>\n",
       "      <td>0.028807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.035059</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.566279</td>\n",
       "      <td>0.028807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.035283</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.566171</td>\n",
       "      <td>0.021872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.035704</td>\n",
       "      <td>0.007738</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.566171</td>\n",
       "      <td>0.021872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.041470</td>\n",
       "      <td>0.009682</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.565433</td>\n",
       "      <td>0.040141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.259484</td>\n",
       "      <td>0.034766</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563912</td>\n",
       "      <td>0.014436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.258985</td>\n",
       "      <td>0.034394</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563844</td>\n",
       "      <td>0.014423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.037248</td>\n",
       "      <td>0.007867</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563420</td>\n",
       "      <td>0.030479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.036374</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563420</td>\n",
       "      <td>0.030479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.038421</td>\n",
       "      <td>0.007616</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563165</td>\n",
       "      <td>0.032659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.036866</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563165</td>\n",
       "      <td>0.032659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.037093</td>\n",
       "      <td>0.007607</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.561849</td>\n",
       "      <td>0.034809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.037216</td>\n",
       "      <td>0.007656</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.561849</td>\n",
       "      <td>0.034809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.037140</td>\n",
       "      <td>0.007849</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.561849</td>\n",
       "      <td>0.034809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.037443</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.561849</td>\n",
       "      <td>0.034809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.037034</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.561849</td>\n",
       "      <td>0.034809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.037734</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.561849</td>\n",
       "      <td>0.034809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>11.125481</td>\n",
       "      <td>1.687183</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.561822</td>\n",
       "      <td>0.003557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.037390</td>\n",
       "      <td>0.007516</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.561253</td>\n",
       "      <td>0.035223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.038069</td>\n",
       "      <td>0.008022</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.561253</td>\n",
       "      <td>0.035223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.037469</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.560339</td>\n",
       "      <td>0.034676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.037246</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.560339</td>\n",
       "      <td>0.034676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.260571</td>\n",
       "      <td>0.035870</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.560143</td>\n",
       "      <td>0.015553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.261888</td>\n",
       "      <td>0.036484</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.560143</td>\n",
       "      <td>0.015553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.263347</td>\n",
       "      <td>0.036411</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.558919</td>\n",
       "      <td>0.015374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.264824</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.558919</td>\n",
       "      <td>0.015374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.262129</td>\n",
       "      <td>0.035689</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557316</td>\n",
       "      <td>0.015118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.270243</td>\n",
       "      <td>0.036620</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557294</td>\n",
       "      <td>0.015115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.264935</td>\n",
       "      <td>0.037015</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557294</td>\n",
       "      <td>0.015115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.262552</td>\n",
       "      <td>0.035753</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557294</td>\n",
       "      <td>0.015115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.266615</td>\n",
       "      <td>0.035933</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557294</td>\n",
       "      <td>0.015115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.262647</td>\n",
       "      <td>0.036407</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557193</td>\n",
       "      <td>0.015118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.565541</td>\n",
       "      <td>0.258134</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.556675</td>\n",
       "      <td>0.011984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032631</td>\n",
       "      <td>0.007620</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550232</td>\n",
       "      <td>0.025280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.036224</td>\n",
       "      <td>0.015753</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.537506</td>\n",
       "      <td>0.030206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.037335</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.537015</td>\n",
       "      <td>0.030992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.046988</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.520964</td>\n",
       "      <td>0.051211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.030049</td>\n",
       "      <td>0.005552</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.520078</td>\n",
       "      <td>0.037188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.030001</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.518243</td>\n",
       "      <td>0.034961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.041127</td>\n",
       "      <td>0.009383</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516677</td>\n",
       "      <td>0.028148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.026304</td>\n",
       "      <td>0.005468</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.515313</td>\n",
       "      <td>0.028204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.020084</td>\n",
       "      <td>0.005661</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.511545</td>\n",
       "      <td>0.041252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.037783</td>\n",
       "      <td>0.015015</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509977</td>\n",
       "      <td>0.011320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.018546</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>scale</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.509880</td>\n",
       "      <td>0.041716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.015749</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509834</td>\n",
       "      <td>0.010566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.041786</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.508427</td>\n",
       "      <td>0.026766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.018986</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.507457</td>\n",
       "      <td>0.037375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.017443</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>5</td>\n",
       "      <td>scale</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.505520</td>\n",
       "      <td>0.041432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.025874</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.505233</td>\n",
       "      <td>0.020932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.038098</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500936</td>\n",
       "      <td>0.023570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.038782</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500391</td>\n",
       "      <td>0.024556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.037397</td>\n",
       "      <td>0.009318</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.496005</td>\n",
       "      <td>0.022982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.037085</td>\n",
       "      <td>0.009468</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495372</td>\n",
       "      <td>0.018894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time param_svc__C param_svc__kernel  \\\n",
       "96      18.292804         5.607623            1               rbf   \n",
       "93       3.065523         0.809643            1               rbf   \n",
       "92       2.118250         0.294197            1              poly   \n",
       "91       2.211192         0.769076           10               rbf   \n",
       "84       0.287028         0.097163            1               rbf   \n",
       "90       3.199683         0.966518            1               rbf   \n",
       "87       2.716950         0.327062            1              poly   \n",
       "86       2.263081         0.377658           10              poly   \n",
       "85       0.284258         0.036106            1              poly   \n",
       "16       0.042063         0.007360            1              poly   \n",
       "48       0.030972         0.013612            1               rbf   \n",
       "56       0.045521         0.013454           10               rbf   \n",
       "83       0.325985         0.095508           10               rbf   \n",
       "24       0.034146         0.006446           10              poly   \n",
       "88       1.752507         0.293310            1              poly   \n",
       "80       0.329916         0.111168            1               rbf   \n",
       "94      11.988793         3.884694           10               rbf   \n",
       "0        0.056638         0.010551            1              poly   \n",
       "73       0.272696         0.034773           10              poly   \n",
       "79       0.272144         0.039011            1              poly   \n",
       "50       0.034453         0.015155            1               rbf   \n",
       "20       0.040192         0.007089            1              poly   \n",
       "81       0.268680         0.040916            1              poly   \n",
       "28       0.033676         0.007318           10              poly   \n",
       "78       0.293910         0.047887           10              poly   \n",
       "18       0.035482         0.007975            1              poly   \n",
       "42       0.034725         0.007688           10              poly   \n",
       "19       0.034350         0.006736            1              poly   \n",
       "43       0.035111         0.006443           10              poly   \n",
       "40       0.048804         0.006199           10              poly   \n",
       "27       0.035159         0.006589           10              poly   \n",
       "3        0.035051         0.006629            1              poly   \n",
       "2        0.037916         0.006887            1              poly   \n",
       "26       0.034064         0.006584           10              poly   \n",
       "25       0.035377         0.007003           10              poly   \n",
       "1        0.038150         0.008470            1              poly   \n",
       "82       0.273600         0.038491           10              poly   \n",
       "64       0.362969         0.038547           10              poly   \n",
       "41       0.037864         0.006231           10              poly   \n",
       "17       0.037360         0.006505            1              poly   \n",
       "44       0.040367         0.006866           10              poly   \n",
       "11       0.034836         0.007184            1              poly   \n",
       "35       0.034049         0.006763           10              poly   \n",
       "58       0.035467         0.015263           10               rbf   \n",
       "34       0.034170         0.006691           10              poly   \n",
       "10       0.035059         0.006903            1              poly   \n",
       "21       0.035283         0.008042            1              poly   \n",
       "45       0.035704         0.007738           10              poly   \n",
       "49       0.041470         0.009682            1           sigmoid   \n",
       "65       0.259484         0.034766            1              poly   \n",
       "66       0.258985         0.034394           10              poly   \n",
       "46       0.037248         0.007867           10              poly   \n",
       "22       0.036374         0.007964            1              poly   \n",
       "23       0.038421         0.007616            1              poly   \n",
       "47       0.036866         0.007282           10              poly   \n",
       "31       0.037093         0.007607           10              poly   \n",
       "5        0.037216         0.007656            1              poly   \n",
       "7        0.037140         0.007849            1              poly   \n",
       "6        0.037443         0.007814            1              poly   \n",
       "30       0.037034         0.007661           10              poly   \n",
       "29       0.037734         0.007957           10              poly   \n",
       "95      11.125481         1.687183            1              poly   \n",
       "39       0.037390         0.007516           10              poly   \n",
       "15       0.038069         0.008022            1              poly   \n",
       "14       0.037469         0.007300            1              poly   \n",
       "38       0.037246         0.007373           10              poly   \n",
       "77       0.260571         0.035870            1              poly   \n",
       "76       0.261888         0.036484           10              poly   \n",
       "75       0.263347         0.036411            1              poly   \n",
       "74       0.264824         0.036499           10              poly   \n",
       "67       0.262129         0.035689           10              poly   \n",
       "72       0.270243         0.036620           10              poly   \n",
       "69       0.264935         0.037015            1              poly   \n",
       "70       0.262552         0.035753            1              poly   \n",
       "71       0.266615         0.035933           10              poly   \n",
       "68       0.262647         0.036407            1              poly   \n",
       "89       1.565541         0.258134           10              poly   \n",
       "4        0.032631         0.007620            1              poly   \n",
       "52       0.036224         0.015753            1               rbf   \n",
       "60       0.037335         0.014673           10               rbf   \n",
       "57       0.046988         0.009064           10           sigmoid   \n",
       "37       0.030049         0.005552           10              poly   \n",
       "13       0.030001         0.004755            1              poly   \n",
       "59       0.041127         0.009383           10           sigmoid   \n",
       "9        0.026304         0.005468            1              poly   \n",
       "8        0.020084         0.005661            1              poly   \n",
       "62       0.037783         0.015015           10               rbf   \n",
       "32       0.018546         0.004967           10              poly   \n",
       "54       0.036530         0.015749            1               rbf   \n",
       "51       0.041786         0.009756            1           sigmoid   \n",
       "12       0.018986         0.005473            1              poly   \n",
       "36       0.017443         0.005255           10              poly   \n",
       "33       0.025874         0.004501           10              poly   \n",
       "61       0.038098         0.009088           10           sigmoid   \n",
       "53       0.038782         0.009896            1           sigmoid   \n",
       "63       0.037397         0.009318           10           sigmoid   \n",
       "55       0.037085         0.009468            1           sigmoid   \n",
       "\n",
       "   param_svc__degree param_svc__gamma param_svc__coef0  mean_test_score  \\\n",
       "96               NaN            scale              NaN         0.634268   \n",
       "93               NaN            scale              NaN         0.626097   \n",
       "92                 3            scale                1         0.616236   \n",
       "91               NaN            scale              NaN         0.612662   \n",
       "84               NaN            scale              NaN         0.603588   \n",
       "90               NaN              0.1              NaN         0.602853   \n",
       "87                 3            scale                0         0.598732   \n",
       "86                 5            scale                0         0.592111   \n",
       "85                 3            scale                1         0.589607   \n",
       "16                 3            scale                1         0.589307   \n",
       "48               NaN            scale              NaN         0.588685   \n",
       "56               NaN            scale              NaN         0.588274   \n",
       "83               NaN            scale              NaN         0.588262   \n",
       "24                 3            scale                0         0.586753   \n",
       "88                 5            scale                1         0.586141   \n",
       "80               NaN              0.1              NaN         0.584069   \n",
       "94               NaN            scale              NaN         0.581595   \n",
       "0                  3            scale                0         0.577658   \n",
       "73                 3            scale                1         0.577536   \n",
       "79                 5            scale                1         0.574795   \n",
       "50               NaN              0.1              NaN         0.574287   \n",
       "20                 5            scale                1         0.573865   \n",
       "81                 3            scale                0         0.572715   \n",
       "28                 5            scale                0         0.572277   \n",
       "78                 5            scale                0         0.571733   \n",
       "18                 3                1                1         0.571318   \n",
       "42                 3                1                1         0.571318   \n",
       "19                 3                2                1         0.570531   \n",
       "43                 3                2                1         0.570531   \n",
       "40                 3            scale                1         0.570379   \n",
       "27                 3                2                0         0.569835   \n",
       "3                  3                2                0         0.569835   \n",
       "2                  3                1                0         0.569835   \n",
       "26                 3                1                0         0.569835   \n",
       "25                 3              0.1                0         0.569816   \n",
       "1                  3              0.1                0         0.569816   \n",
       "82                 3            scale                0         0.569570   \n",
       "64                 5            scale                1         0.568876   \n",
       "41                 3              0.1                1         0.568844   \n",
       "17                 3              0.1                1         0.568844   \n",
       "44                 5            scale                1         0.568722   \n",
       "11                 3                2               -1         0.568341   \n",
       "35                 3                2               -1         0.568341   \n",
       "58               NaN              0.1              NaN         0.567325   \n",
       "34                 3                1               -1         0.566279   \n",
       "10                 3                1               -1         0.566279   \n",
       "21                 5              0.1                1         0.566171   \n",
       "45                 5              0.1                1         0.566171   \n",
       "49               NaN            scale              NaN         0.565433   \n",
       "65                 3              0.1                1         0.563912   \n",
       "66                 3              0.1                1         0.563844   \n",
       "46                 5                1                1         0.563420   \n",
       "22                 5                1                1         0.563420   \n",
       "23                 5                2                1         0.563165   \n",
       "47                 5                2                1         0.563165   \n",
       "31                 5                2                0         0.561849   \n",
       "5                  5              0.1                0         0.561849   \n",
       "7                  5                2                0         0.561849   \n",
       "6                  5                1                0         0.561849   \n",
       "30                 5                1                0         0.561849   \n",
       "29                 5              0.1                0         0.561849   \n",
       "95                 3            scale                1         0.561822   \n",
       "39                 5                2               -1         0.561253   \n",
       "15                 5                2               -1         0.561253   \n",
       "14                 5                1               -1         0.560339   \n",
       "38                 5                1               -1         0.560339   \n",
       "77                 3                1                1         0.560143   \n",
       "76                 3                1                1         0.560143   \n",
       "75                 3                2                1         0.558919   \n",
       "74                 3                2                1         0.558919   \n",
       "67                 3              0.1                0         0.557316   \n",
       "72                 3                1                0         0.557294   \n",
       "69                 3                2                0         0.557294   \n",
       "70                 3                1                0         0.557294   \n",
       "71                 3                2                0         0.557294   \n",
       "68                 3              0.1                0         0.557193   \n",
       "89                 3            scale                1         0.556675   \n",
       "4                  5            scale                0         0.550232   \n",
       "52               NaN                1              NaN         0.537506   \n",
       "60               NaN                1              NaN         0.537015   \n",
       "57               NaN            scale              NaN         0.520964   \n",
       "37                 5              0.1               -1         0.520078   \n",
       "13                 5              0.1               -1         0.518243   \n",
       "59               NaN              0.1              NaN         0.516677   \n",
       "9                  3              0.1               -1         0.515313   \n",
       "8                  3            scale               -1         0.511545   \n",
       "62               NaN                2              NaN         0.509977   \n",
       "32                 3            scale               -1         0.509880   \n",
       "54               NaN                2              NaN         0.509834   \n",
       "51               NaN              0.1              NaN         0.508427   \n",
       "12                 5            scale               -1         0.507457   \n",
       "36                 5            scale               -1         0.505520   \n",
       "33                 3              0.1               -1         0.505233   \n",
       "61               NaN                1              NaN         0.500936   \n",
       "53               NaN                1              NaN         0.500391   \n",
       "63               NaN                2              NaN         0.496005   \n",
       "55               NaN                2              NaN         0.495372   \n",
       "\n",
       "    std_test_score  \n",
       "96        0.002076  \n",
       "93        0.014080  \n",
       "92        0.011990  \n",
       "91        0.012683  \n",
       "84        0.021176  \n",
       "90        0.010565  \n",
       "87        0.012288  \n",
       "86        0.010182  \n",
       "85        0.017931  \n",
       "16        0.030650  \n",
       "48        0.025528  \n",
       "56        0.027045  \n",
       "83        0.020385  \n",
       "24        0.027863  \n",
       "88        0.011901  \n",
       "80        0.021823  \n",
       "94        0.007319  \n",
       "0         0.021641  \n",
       "73        0.008328  \n",
       "79        0.016676  \n",
       "50        0.015040  \n",
       "20        0.024639  \n",
       "81        0.018665  \n",
       "28        0.022845  \n",
       "78        0.019988  \n",
       "18        0.029297  \n",
       "42        0.029297  \n",
       "19        0.029023  \n",
       "43        0.029023  \n",
       "40        0.035869  \n",
       "27        0.029430  \n",
       "3         0.029430  \n",
       "2         0.029430  \n",
       "26        0.029430  \n",
       "25        0.029425  \n",
       "1         0.029425  \n",
       "82        0.019304  \n",
       "64        0.015345  \n",
       "41        0.029519  \n",
       "17        0.029519  \n",
       "44        0.027811  \n",
       "11        0.029473  \n",
       "35        0.029473  \n",
       "58        0.015858  \n",
       "34        0.028807  \n",
       "10        0.028807  \n",
       "21        0.021872  \n",
       "45        0.021872  \n",
       "49        0.040141  \n",
       "65        0.014436  \n",
       "66        0.014423  \n",
       "46        0.030479  \n",
       "22        0.030479  \n",
       "23        0.032659  \n",
       "47        0.032659  \n",
       "31        0.034809  \n",
       "5         0.034809  \n",
       "7         0.034809  \n",
       "6         0.034809  \n",
       "30        0.034809  \n",
       "29        0.034809  \n",
       "95        0.003557  \n",
       "39        0.035223  \n",
       "15        0.035223  \n",
       "14        0.034676  \n",
       "38        0.034676  \n",
       "77        0.015553  \n",
       "76        0.015553  \n",
       "75        0.015374  \n",
       "74        0.015374  \n",
       "67        0.015118  \n",
       "72        0.015115  \n",
       "69        0.015115  \n",
       "70        0.015115  \n",
       "71        0.015115  \n",
       "68        0.015118  \n",
       "89        0.011984  \n",
       "4         0.025280  \n",
       "52        0.030206  \n",
       "60        0.030992  \n",
       "57        0.051211  \n",
       "37        0.037188  \n",
       "13        0.034961  \n",
       "59        0.028148  \n",
       "9         0.028204  \n",
       "8         0.041252  \n",
       "62        0.011320  \n",
       "32        0.041716  \n",
       "54        0.010566  \n",
       "51        0.026766  \n",
       "12        0.037375  \n",
       "36        0.041432  \n",
       "33        0.020932  \n",
       "61        0.023570  \n",
       "53        0.024556  \n",
       "63        0.022982  \n",
       "55        0.018894  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_grid_df = pd.DataFrame(svc_grid.cv_results_)\n",
    "svc_grid_df.sort_values('mean_test_score', ascending=False)[['mean_fit_time', 'mean_score_time', 'param_svc__C', 'param_svc__kernel', 'param_svc__degree', 'param_svc__gamma', 'param_svc__coef0', 'mean_test_score', 'std_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5694363507607965, 0.6377)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_svc = svc_grid.predict(X_test)\n",
    "(roc_auc_score(y_test, y_test_svc), accuracy_score(y_test, y_test_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(svc_grid, 'svc_grid.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risco bayesiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 1481\n",
      "max_resources_: 40000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 28\n",
      "n_resources: 1481\n",
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.662) total time=   1.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.675) total time=   1.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.685) total time=   1.0s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.671) total time=   1.0s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.660) total time=   1.1s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.662) total time=   2.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.678) total time=   2.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.685) total time=   2.1s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.666) total time=   2.2s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.665) total time=   2.1s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.665) total time=   3.2s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.681) total time=   3.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.691) total time=   3.1s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.664) total time=   3.0s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.665) total time=   3.0s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.669) total time=   1.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.676) total time=   1.0s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.693) total time=   1.0s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.664) total time=   1.0s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.672) total time=   1.1s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.675) total time=   2.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.677) total time=   2.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.691) total time=   2.0s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.663) total time=   2.0s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.670) total time=   2.1s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.678) total time=   3.0s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.678) total time=   3.0s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.694) total time=   3.0s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.664) total time=   3.1s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.670) total time=   2.9s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.658) total time=   1.0s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.672) total time=   1.0s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.694) total time=   1.0s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.655) total time=   1.0s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.668) total time=   1.1s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.666) total time=   2.0s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.668) total time=   2.1s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.694) total time=   2.1s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.659) total time=   2.1s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.664) total time=   2.1s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.665) total time=   3.2s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.673) total time=   3.1s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.694) total time=   3.1s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.658) total time=   3.2s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.661) total time=   3.3s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.678) total time=   1.1s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.677) total time=   1.1s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.691) total time=   1.1s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.667) total time=   1.0s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.665) total time=   1.0s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.687) total time=   2.0s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.674) total time=   2.0s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.695) total time=   2.0s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.668) total time=   2.0s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.661) total time=   2.0s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.685) total time=   2.9s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.677) total time=   3.0s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.695) total time=   2.9s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.664) total time=   3.0s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.663) total time=   3.0s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.593) total time=   2.3s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.591) total time=   2.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.664) total time=   2.1s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.581) total time=   2.1s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.564) total time=   2.1s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.617) total time=   2.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.642) total time=   2.2s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.682) total time=   2.3s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.640) total time=   2.1s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.633) total time=   2.0s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.589) total time=   3.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.593) total time=   3.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.670) total time=   3.2s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.576) total time=   3.2s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.569) total time=   3.1s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.615) total time=   3.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.643) total time=   3.2s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.680) total time=   3.1s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.636) total time=   3.4s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.637) total time=   3.4s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.589) total time=   2.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.592) total time=   2.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.672) total time=   2.1s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.575) total time=   2.1s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.574) total time=   2.1s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.621) total time=   2.4s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.638) total time=   2.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.675) total time=   2.2s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.635) total time=   2.0s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.630) total time=   2.0s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.594) total time=   3.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.592) total time=   3.0s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.672) total time=   3.0s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.577) total time=   3.0s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.578) total time=   3.0s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.624) total time=   3.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.635) total time=   3.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.677) total time=   3.3s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.636) total time=   3.3s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.630) total time=   3.1s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.593) total time=   2.1s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.591) total time=   2.1s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.664) total time=   2.1s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.581) total time=   2.1s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.564) total time=   2.1s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.630) total time=   2.3s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.639) total time=   2.2s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.689) total time=   2.3s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.645) total time=   2.1s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.632) total time=   2.0s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.589) total time=   3.1s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.593) total time=   3.1s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.670) total time=   3.0s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.576) total time=   3.1s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.569) total time=   3.1s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.630) total time=   3.2s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.641) total time=   3.3s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.686) total time=   3.3s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.646) total time=   3.4s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.638) total time=   3.3s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.589) total time=   2.2s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.592) total time=   2.0s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.672) total time=   2.1s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.575) total time=   2.1s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.574) total time=   2.1s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.628) total time=   2.1s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.641) total time=   2.3s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.670) total time=   2.5s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.635) total time=   2.2s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.629) total time=   2.2s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.594) total time=   3.0s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.592) total time=   3.0s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.672) total time=   3.0s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.577) total time=   3.1s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=10;, score=(train=1.000, test=0.578) total time=   3.0s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.624) total time=   3.3s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.644) total time=   3.2s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.673) total time=   3.1s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.639) total time=   3.1s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=PCA(random_state=1729), pca__n_components=30;, score=(train=1.000, test=0.636) total time=   3.1s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 10\n",
      "n_resources: 4443\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.670) total time=   3.4s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.675) total time=   3.5s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.687) total time=   3.4s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.692) total time=   3.4s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.668) total time=   3.4s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.663) total time=   1.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.666) total time=   1.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.678) total time=   1.1s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.699) total time=   1.1s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.684) total time=   1.1s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.664) total time=   2.3s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.672) total time=   2.3s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.686) total time=   2.3s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.693) total time=   2.3s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.679) total time=   2.4s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.665) total time=   3.8s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.672) total time=   3.5s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.685) total time=   3.5s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.696) total time=   3.5s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.679) total time=   3.5s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.665) total time=   1.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.666) total time=   1.2s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.686) total time=   1.2s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.693) total time=   1.1s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.670) total time=   1.1s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.664) total time=   2.4s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.671) total time=   2.3s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.686) total time=   2.2s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.693) total time=   2.2s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.671) total time=   2.2s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.667) total time=   1.2s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.669) total time=   1.1s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.681) total time=   1.1s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.689) total time=   1.1s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.670) total time=   1.1s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.667) total time=   3.4s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.670) total time=   3.4s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.682) total time=   3.4s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.692) total time=   3.4s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.672) total time=   3.3s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.668) total time=   2.2s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.664) total time=   2.2s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.683) total time=   2.2s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.692) total time=   2.2s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.670) total time=   2.2s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.668) total time=   3.4s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.668) total time=   3.3s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.683) total time=   3.5s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.690) total time=   3.4s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=10, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.669) total time=   3.4s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 4\n",
      "n_resources: 13329\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.702) total time=   1.5s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.707) total time=   1.5s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.734) total time=   1.5s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.715) total time=   1.5s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=500, pca=passthrough;, score=(train=1.000, test=0.717) total time=   1.5s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.706) total time=   3.1s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.709) total time=   3.1s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.732) total time=   3.1s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.716) total time=   3.0s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1000, pca=passthrough;, score=(train=1.000, test=0.718) total time=   3.0s\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.707) total time=   4.4s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.710) total time=   4.4s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.731) total time=   4.3s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.719) total time=   4.3s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.718) total time=   4.3s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.707) total time=   4.4s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.712) total time=   4.4s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.733) total time=   4.4s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.719) total time=   4.4s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.718) total time=   4.6s\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 2\n",
      "n_resources: 39987\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.781) total time=   6.3s\n",
      "[CV 2/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.776) total time=   6.2s\n",
      "[CV 3/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.781) total time=   6.3s\n",
      "[CV 4/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.781) total time=   6.4s\n",
      "[CV 5/5] END forest__max_features=log2, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.779) total time=   6.3s\n",
      "[CV 1/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.784) total time=   7.0s\n",
      "[CV 2/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.779) total time=   6.9s\n",
      "[CV 3/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.784) total time=   7.0s\n",
      "[CV 4/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.784) total time=   6.7s\n",
      "[CV 5/5] END forest__max_features=sqrt, forest__min_samples_split=2, forest__n_estimators=1500, pca=passthrough;, score=(train=1.000, test=0.783) total time=   6.8s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(estimator=Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=1729)),\n",
       "                                              (&#x27;forest&#x27;,\n",
       "                                               RandomForestClassifier(n_jobs=-1,\n",
       "                                                                      random_state=1729))]),\n",
       "                    n_jobs=1,\n",
       "                    param_grid=[{&#x27;forest__max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                                 &#x27;forest__min_samples_split&#x27;: [2, 10],\n",
       "                                 &#x27;forest__n_estimators&#x27;: [500, 1000, 1500],\n",
       "                                 &#x27;pca&#x27;: [&#x27;passthrough&#x27;]},\n",
       "                                {&#x27;forest__max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                                 &#x27;forest__min_samples_split&#x27;: [2, 10],\n",
       "                                 &#x27;forest__n_estimators&#x27;: [1000, 1500],\n",
       "                                 &#x27;pca&#x27;: [PCA(random_state=1729)],\n",
       "                                 &#x27;pca__n_components&#x27;: [10, 30]}],\n",
       "                    scoring=&#x27;roc_auc&#x27;, verbose=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(estimator=Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=1729)),\n",
       "                                              (&#x27;forest&#x27;,\n",
       "                                               RandomForestClassifier(n_jobs=-1,\n",
       "                                                                      random_state=1729))]),\n",
       "                    n_jobs=1,\n",
       "                    param_grid=[{&#x27;forest__max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                                 &#x27;forest__min_samples_split&#x27;: [2, 10],\n",
       "                                 &#x27;forest__n_estimators&#x27;: [500, 1000, 1500],\n",
       "                                 &#x27;pca&#x27;: [&#x27;passthrough&#x27;]},\n",
       "                                {&#x27;forest__max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                                 &#x27;forest__min_samples_split&#x27;: [2, 10],\n",
       "                                 &#x27;forest__n_estimators&#x27;: [1000, 1500],\n",
       "                                 &#x27;pca&#x27;: [PCA(random_state=1729)],\n",
       "                                 &#x27;pca__n_components&#x27;: [10, 30]}],\n",
       "                    scoring=&#x27;roc_auc&#x27;, verbose=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=1729)),\n",
       "                (&#x27;forest&#x27;,\n",
       "                 RandomForestClassifier(n_jobs=-1, random_state=1729))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(random_state=1729)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1, random_state=1729)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(estimator=Pipeline(steps=[('pca', PCA(random_state=1729)),\n",
       "                                              ('forest',\n",
       "                                               RandomForestClassifier(n_jobs=-1,\n",
       "                                                                      random_state=1729))]),\n",
       "                    n_jobs=1,\n",
       "                    param_grid=[{'forest__max_features': ['sqrt', 'log2'],\n",
       "                                 'forest__min_samples_split': [2, 10],\n",
       "                                 'forest__n_estimators': [500, 1000, 1500],\n",
       "                                 'pca': ['passthrough']},\n",
       "                                {'forest__max_features': ['sqrt', 'log2'],\n",
       "                                 'forest__min_samples_split': [2, 10],\n",
       "                                 'forest__n_estimators': [1000, 1500],\n",
       "                                 'pca': [PCA(random_state=1729)],\n",
       "                                 'pca__n_components': [10, 30]}],\n",
       "                    scoring='roc_auc', verbose=4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_params = [\n",
    "    {\n",
    "        'pca': ['passthrough'],\n",
    "        'forest__n_estimators': [500, 1000, 1500],\n",
    "        'forest__min_samples_split': [2, 10],\n",
    "        'forest__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    {\n",
    "        'pca': [PCA(random_state=1729)],\n",
    "        'pca__n_components': [10, 30],\n",
    "        'forest__n_estimators': [1000,1500],\n",
    "        'forest__min_samples_split': [2, 10],\n",
    "        'forest__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "forest_grid = HalvingGridSearchCV(\n",
    "    Pipeline(steps=[('pca', PCA(random_state=1729)), ('forest', RandomForestClassifier(random_state=1729, n_jobs=-1))]),\n",
    "    forest_params,\n",
    "    verbose=4,\n",
    "    scoring='roc_auc',\n",
    "    refit=True,\n",
    "    n_jobs=1)\n",
    "forest_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>param_pca__n_components</th>\n",
       "      <th>param_forest__n_estimators</th>\n",
       "      <th>param_forest__min_samples_split</th>\n",
       "      <th>param_forest__max_features</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6.400563</td>\n",
       "      <td>0.493930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.782919</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5.801655</td>\n",
       "      <td>0.484686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.779450</td>\n",
       "      <td>0.001714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.029474</td>\n",
       "      <td>0.425384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.717765</td>\n",
       "      <td>0.008806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3.892315</td>\n",
       "      <td>0.445826</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.717091</td>\n",
       "      <td>0.008464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.730756</td>\n",
       "      <td>0.298774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.716434</td>\n",
       "      <td>0.009236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.358066</td>\n",
       "      <td>0.154230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.714895</td>\n",
       "      <td>0.010768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.143049</td>\n",
       "      <td>0.425715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.679363</td>\n",
       "      <td>0.010691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.998700</td>\n",
       "      <td>0.415756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.678665</td>\n",
       "      <td>0.009431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.045220</td>\n",
       "      <td>0.292081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.678582</td>\n",
       "      <td>0.010141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.997731</td>\n",
       "      <td>0.140392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.678080</td>\n",
       "      <td>0.012981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.984908</td>\n",
       "      <td>0.292987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.677209</td>\n",
       "      <td>0.010821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.652245</td>\n",
       "      <td>0.308210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.676986</td>\n",
       "      <td>0.012378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.778041</td>\n",
       "      <td>0.210777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.676881</td>\n",
       "      <td>0.012476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.694737</td>\n",
       "      <td>0.314325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.676852</td>\n",
       "      <td>0.009887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2.945736</td>\n",
       "      <td>0.419652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.676815</td>\n",
       "      <td>0.009384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.007965</td>\n",
       "      <td>0.143031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.675975</td>\n",
       "      <td>0.011481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.970231</td>\n",
       "      <td>0.435394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.675777</td>\n",
       "      <td>0.009212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.934307</td>\n",
       "      <td>0.118276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.675765</td>\n",
       "      <td>0.009346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.913678</td>\n",
       "      <td>0.285252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.675328</td>\n",
       "      <td>0.010240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.823544</td>\n",
       "      <td>0.224860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.675194</td>\n",
       "      <td>0.009371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.988317</td>\n",
       "      <td>0.145522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.675072</td>\n",
       "      <td>0.008572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.938243</td>\n",
       "      <td>0.115816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.674774</td>\n",
       "      <td>0.009710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.759832</td>\n",
       "      <td>0.317227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.673415</td>\n",
       "      <td>0.011029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.884354</td>\n",
       "      <td>0.214580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.671158</td>\n",
       "      <td>0.008869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.941629</td>\n",
       "      <td>0.113910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.670658</td>\n",
       "      <td>0.009224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.861092</td>\n",
       "      <td>0.320987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.670281</td>\n",
       "      <td>0.012862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.848649</td>\n",
       "      <td>0.216331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.670145</td>\n",
       "      <td>0.012122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.921686</td>\n",
       "      <td>0.111344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.669562</td>\n",
       "      <td>0.013513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.919920</td>\n",
       "      <td>0.374782</td>\n",
       "      <td>30</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.648242</td>\n",
       "      <td>0.019482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.936397</td>\n",
       "      <td>0.241050</td>\n",
       "      <td>30</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.646872</td>\n",
       "      <td>0.021492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.815266</td>\n",
       "      <td>0.355930</td>\n",
       "      <td>30</td>\n",
       "      <td>1500</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.643264</td>\n",
       "      <td>0.016466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.964631</td>\n",
       "      <td>0.197854</td>\n",
       "      <td>30</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.642673</td>\n",
       "      <td>0.021601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.911290</td>\n",
       "      <td>0.332957</td>\n",
       "      <td>30</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.642106</td>\n",
       "      <td>0.020992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.930550</td>\n",
       "      <td>0.314626</td>\n",
       "      <td>30</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.640640</td>\n",
       "      <td>0.015355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.817961</td>\n",
       "      <td>0.355117</td>\n",
       "      <td>30</td>\n",
       "      <td>1500</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.640468</td>\n",
       "      <td>0.018825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.931793</td>\n",
       "      <td>0.214093</td>\n",
       "      <td>30</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.640127</td>\n",
       "      <td>0.018656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.716393</td>\n",
       "      <td>0.303276</td>\n",
       "      <td>10</td>\n",
       "      <td>1500</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.602471</td>\n",
       "      <td>0.035304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.722388</td>\n",
       "      <td>0.308525</td>\n",
       "      <td>10</td>\n",
       "      <td>1500</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.602471</td>\n",
       "      <td>0.035304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.860216</td>\n",
       "      <td>0.207501</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.600413</td>\n",
       "      <td>0.036576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.903094</td>\n",
       "      <td>0.218827</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.600413</td>\n",
       "      <td>0.036576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.819232</td>\n",
       "      <td>0.316454</td>\n",
       "      <td>10</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.599418</td>\n",
       "      <td>0.036383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.767130</td>\n",
       "      <td>0.305895</td>\n",
       "      <td>10</td>\n",
       "      <td>1500</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.599418</td>\n",
       "      <td>0.036383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.925200</td>\n",
       "      <td>0.212152</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.598487</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.882250</td>\n",
       "      <td>0.205602</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>0.598487</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time param_pca__n_components  \\\n",
       "43       6.400563         0.493930                     NaN   \n",
       "42       5.801655         0.484686                     NaN   \n",
       "41       4.029474         0.425384                     NaN   \n",
       "40       3.892315         0.445826                     NaN   \n",
       "39       2.730756         0.298774                     NaN   \n",
       "38       1.358066         0.154230                     NaN   \n",
       "31       3.143049         0.425715                     NaN   \n",
       "28       2.998700         0.415756                     NaN   \n",
       "30       2.045220         0.292081                     NaN   \n",
       "29       0.997731         0.140392                     NaN   \n",
       "33       1.984908         0.292987                     NaN   \n",
       "11       2.652245         0.308210                     NaN   \n",
       "10       1.778041         0.210777                     NaN   \n",
       "5        2.694737         0.314325                     NaN   \n",
       "35       2.945736         0.419652                     NaN   \n",
       "32       1.007965         0.143031                     NaN   \n",
       "37       2.970231         0.435394                     NaN   \n",
       "9        0.934307         0.118276                     NaN   \n",
       "36       1.913678         0.285252                     NaN   \n",
       "4        1.823544         0.224860                     NaN   \n",
       "34       0.988317         0.145522                     NaN   \n",
       "3        0.938243         0.115816                     NaN   \n",
       "2        2.759832         0.317227                     NaN   \n",
       "1        1.884354         0.214580                     NaN   \n",
       "0        0.941629         0.113910                     NaN   \n",
       "8        2.861092         0.320987                     NaN   \n",
       "7        1.848649         0.216331                     NaN   \n",
       "6        0.921686         0.111344                     NaN   \n",
       "23       2.919920         0.374782                      30   \n",
       "21       1.936397         0.241050                      30   \n",
       "27       2.815266         0.355930                      30   \n",
       "13       1.964631         0.197854                      30   \n",
       "15       2.911290         0.332957                      30   \n",
       "25       1.930550         0.314626                      30   \n",
       "19       2.817961         0.355117                      30   \n",
       "17       1.931793         0.214093                      30   \n",
       "26       2.716393         0.303276                      10   \n",
       "18       2.722388         0.308525                      10   \n",
       "16       1.860216         0.207501                      10   \n",
       "24       1.903094         0.218827                      10   \n",
       "14       2.819232         0.316454                      10   \n",
       "22       2.767130         0.305895                      10   \n",
       "12       1.925200         0.212152                      10   \n",
       "20       1.882250         0.205602                      10   \n",
       "\n",
       "   param_forest__n_estimators param_forest__min_samples_split  \\\n",
       "43                       1500                               2   \n",
       "42                       1500                               2   \n",
       "41                       1500                               2   \n",
       "40                       1500                               2   \n",
       "39                       1000                               2   \n",
       "38                        500                               2   \n",
       "31                       1500                               2   \n",
       "28                       1500                               2   \n",
       "30                       1000                               2   \n",
       "29                        500                               2   \n",
       "33                       1000                              10   \n",
       "11                       1500                              10   \n",
       "10                       1000                              10   \n",
       "5                        1500                              10   \n",
       "35                       1500                              10   \n",
       "32                        500                              10   \n",
       "37                       1500                              10   \n",
       "9                         500                              10   \n",
       "36                       1000                              10   \n",
       "4                        1000                              10   \n",
       "34                        500                              10   \n",
       "3                         500                              10   \n",
       "2                        1500                               2   \n",
       "1                        1000                               2   \n",
       "0                         500                               2   \n",
       "8                        1500                               2   \n",
       "7                        1000                               2   \n",
       "6                         500                               2   \n",
       "23                       1500                               2   \n",
       "21                       1000                               2   \n",
       "27                       1500                              10   \n",
       "13                       1000                               2   \n",
       "15                       1500                               2   \n",
       "25                       1000                              10   \n",
       "19                       1500                              10   \n",
       "17                       1000                              10   \n",
       "26                       1500                              10   \n",
       "18                       1500                              10   \n",
       "16                       1000                              10   \n",
       "24                       1000                              10   \n",
       "14                       1500                               2   \n",
       "22                       1500                               2   \n",
       "12                       1000                               2   \n",
       "20                       1000                               2   \n",
       "\n",
       "   param_forest__max_features  mean_test_score  std_test_score  \n",
       "43                       sqrt         0.782919        0.001900  \n",
       "42                       log2         0.779450        0.001714  \n",
       "41                       sqrt         0.717765        0.008806  \n",
       "40                       log2         0.717091        0.008464  \n",
       "39                       sqrt         0.716434        0.009236  \n",
       "38                       sqrt         0.714895        0.010768  \n",
       "31                       sqrt         0.679363        0.010691  \n",
       "28                       log2         0.678665        0.009431  \n",
       "30                       sqrt         0.678582        0.010141  \n",
       "29                       sqrt         0.678080        0.012981  \n",
       "33                       sqrt         0.677209        0.010821  \n",
       "11                       log2         0.676986        0.012378  \n",
       "10                       log2         0.676881        0.012476  \n",
       "5                        sqrt         0.676852        0.009887  \n",
       "35                       sqrt         0.676815        0.009384  \n",
       "32                       sqrt         0.675975        0.011481  \n",
       "37                       log2         0.675777        0.009212  \n",
       "9                        log2         0.675765        0.009346  \n",
       "36                       log2         0.675328        0.010240  \n",
       "4                        sqrt         0.675194        0.009371  \n",
       "34                       log2         0.675072        0.008572  \n",
       "3                        sqrt         0.674774        0.009710  \n",
       "2                        sqrt         0.673415        0.011029  \n",
       "1                        sqrt         0.671158        0.008869  \n",
       "0                        sqrt         0.670658        0.009224  \n",
       "8                        log2         0.670281        0.012862  \n",
       "7                        log2         0.670145        0.012122  \n",
       "6                        log2         0.669562        0.013513  \n",
       "23                       log2         0.648242        0.019482  \n",
       "21                       log2         0.646872        0.021492  \n",
       "27                       log2         0.643264        0.016466  \n",
       "13                       sqrt         0.642673        0.021601  \n",
       "15                       sqrt         0.642106        0.020992  \n",
       "25                       log2         0.640640        0.015355  \n",
       "19                       sqrt         0.640468        0.018825  \n",
       "17                       sqrt         0.640127        0.018656  \n",
       "26                       log2         0.602471        0.035304  \n",
       "18                       sqrt         0.602471        0.035304  \n",
       "16                       sqrt         0.600413        0.036576  \n",
       "24                       log2         0.600413        0.036576  \n",
       "14                       sqrt         0.599418        0.036383  \n",
       "22                       log2         0.599418        0.036383  \n",
       "12                       sqrt         0.598487        0.034467  \n",
       "20                       log2         0.598487        0.034467  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_grid_df = pd.DataFrame(forest_grid.cv_results_)\n",
    "forest_grid_df.sort_values('mean_test_score', ascending=False)[['mean_fit_time', 'mean_score_time', 'param_pca__n_components', 'param_forest__n_estimators', 'param_forest__min_samples_split', 'param_forest__max_features', 'mean_test_score', 'std_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=1500, n_jobs=-1, random_state=1729)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=1500, n_jobs=-1, random_state=1729)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=1500, n_jobs=-1, random_state=1729)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_grid = RandomForestClassifier(\n",
    "    n_estimators=1500,\n",
    "    min_samples_split=2,\n",
    "    max_features='sqrt',\n",
    "    random_state=1729,\n",
    "    n_jobs=-1\n",
    ")\n",
    "forest_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7559540584585669, 0.774675)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_forest = forest_grid.predict_proba(X_test)[:,1]\n",
    "y_test_forest_05 = y_test_forest > 0.5\n",
    "(roc_auc_score(y_test, y_test_forest_05), accuracy_score(y_test, y_test_forest_05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forest_grid.model']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(forest_grid, 'forest_grid.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_grid = load('forest_grid.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Risco bayesiano\n",
    "\n",
    "C√°lculo da raz√£o de verossimilhan√ßa. Aqui fazemos a mesma assump√ß√£o do notebook que $\\mathcal{L}(x|Y=y) = \\hat p (y|x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_y_test_forest = y_test_forest / (1 - y_test_forest)\n",
    "L_y_test_forest_1 = (L_y_test_forest > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7559540584585669, 0.774675)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(roc_auc_score(y_test, L_y_test_forest_1), accuracy_score(y_test, L_y_test_forest_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disitribui√ß√£o das classes no dado original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5771"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risco padr√£o: $\\eta = 1$\n",
    "\n",
    "No nosso caso, o problema √© inerentemente sim√©trico. Como existem poucos pacientes transplantados no mesmo dia, podemos desconsider√°-los. Ent√£o, dados dois pacientes quaisquer $p$ e $q$, existe uma ordem absoluta entre $p$ e $q$ e a chance de aleatoriamente escolher $(p, q)$ √© a mesma de escolher $(q, p)$. Logo, o problema √© balanceado.\n",
    "\n",
    "Tamb√©m podemos observar que um falso positivo para $(p, q)$ √© equivalente a um falso negativo para $(q, p)$. Logo, os custos para falsos negativos e positivos devem serem os mesmos e, portanto, o limite bayesiano √© igual ao risco padr√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, tentamos encontrar o limite que aproxima a distribui√ß√£o de classes entre o dado original e o dado previsto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold=1.1865889212831462 rate=0.5764\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "l, r = 0, 2\n",
    "while r - l > 1e-12:\n",
    "    m = (l + r) / 2\n",
    "    if (L_y_test_forest > m).mean() <= y_test.mean():\n",
    "        r = m\n",
    "    else:\n",
    "        l = m\n",
    "    i += 1\n",
    "print(f\"threshold={m} rate={(L_y_test_forest > m).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concord√¢ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qsort(data, compare_many_one):\n",
    "    if len(data) <= 1:\n",
    "        return data\n",
    "    else:\n",
    "        p = random.choice(range(len(data)))\n",
    "        pivot = data[p]\n",
    "        cmp = compare_many_one(data, pivot)\n",
    "        lt, gt = [], []\n",
    "        for i in range(len(data)):\n",
    "            if i == p:\n",
    "                continue\n",
    "            if cmp[i]:\n",
    "                lt.append(data[i])\n",
    "            else:\n",
    "                gt.append(data[i])\n",
    "        data = qsort(lt, compare_many_one)\n",
    "        data.append(pivot)\n",
    "        data.extend(qsort(gt, compare_many_one))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_registered</th>\n",
       "      <th>age_registered</th>\n",
       "      <th>dialysis_session_count</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>chagas</th>\n",
       "      <th>transfusion_count</th>\n",
       "      <th>gestation</th>\n",
       "      <th>prior_transplant</th>\n",
       "      <th>c_pra</th>\n",
       "      <th>hla_a1</th>\n",
       "      <th>hla_a2</th>\n",
       "      <th>hla_b1</th>\n",
       "      <th>hla_b2</th>\n",
       "      <th>hla_dr1</th>\n",
       "      <th>hla_dr2</th>\n",
       "      <th>anti_hbc</th>\n",
       "      <th>anti_hcv</th>\n",
       "      <th>hbs_ag</th>\n",
       "      <th>days_waiting</th>\n",
       "      <th>date_transplanted</th>\n",
       "      <th>sex_M</th>\n",
       "      <th>underlying_disease_glomerulonephritis</th>\n",
       "      <th>underlying_disease_hypertension</th>\n",
       "      <th>underlying_disease_other</th>\n",
       "      <th>underlying_disease_pyelonephritis</th>\n",
       "      <th>blood_type_AB</th>\n",
       "      <th>blood_type_B</th>\n",
       "      <th>blood_type_O</th>\n",
       "      <th>dr_00_homozygous</th>\n",
       "      <th>b_00_homozygous</th>\n",
       "      <th>a_00_homozygous</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12405</th>\n",
       "      <td>16889.0</td>\n",
       "      <td>39</td>\n",
       "      <td>51.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>319</td>\n",
       "      <td>2017-02-11</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>41026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12836</th>\n",
       "      <td>17029.0</td>\n",
       "      <td>12</td>\n",
       "      <td>22.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>284</td>\n",
       "      <td>2017-05-27</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>32477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13411</th>\n",
       "      <td>17312.0</td>\n",
       "      <td>17</td>\n",
       "      <td>9.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>98</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>17954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12820</th>\n",
       "      <td>17021.0</td>\n",
       "      <td>16</td>\n",
       "      <td>9.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>38483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11702</th>\n",
       "      <td>16633.0</td>\n",
       "      <td>16</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>66</td>\n",
       "      <td>14</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>189</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>26210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date_registered  age_registered  dialysis_session_count  diabetes  \\\n",
       "12405          16889.0              39                    51.0     False   \n",
       "12836          17029.0              12                    22.0      True   \n",
       "13411          17312.0              17                     9.0      True   \n",
       "12820          17021.0              16                     9.0      True   \n",
       "11702          16633.0              16                     8.0      True   \n",
       "\n",
       "       chagas  transfusion_count  gestation  prior_transplant  c_pra  hla_a1  \\\n",
       "12405   False                  1      False             False      0       2   \n",
       "12836   False                  1      False             False      0       2   \n",
       "13411   False                  1      False             False      0       2   \n",
       "12820   False                  1      False             False      0      24   \n",
       "11702   False                  0      False             False      0      23   \n",
       "\n",
       "       hla_a2  hla_b1  hla_b2  hla_dr1  hla_dr2  anti_hbc  anti_hcv  hbs_ag  \\\n",
       "12405       3      15       0        1       11      True     False   False   \n",
       "12836      23      40      44        4        9     False     False   False   \n",
       "13411      33       7      15        1        1     False     False   False   \n",
       "12820      30       8      35        1       13     False     False   False   \n",
       "11702      66      14      51        1       11     False     False   False   \n",
       "\n",
       "       days_waiting date_transplanted  sex_M  \\\n",
       "12405           319        2017-02-11   True   \n",
       "12836           284        2017-05-27  False   \n",
       "13411            98        2017-09-01  False   \n",
       "12820            23        2016-08-31   True   \n",
       "11702           189        2016-01-22  False   \n",
       "\n",
       "       underlying_disease_glomerulonephritis  underlying_disease_hypertension  \\\n",
       "12405                                  False                            False   \n",
       "12836                                   True                            False   \n",
       "13411                                  False                            False   \n",
       "12820                                   True                            False   \n",
       "11702                                  False                            False   \n",
       "\n",
       "       underlying_disease_other  underlying_disease_pyelonephritis  \\\n",
       "12405                     False                              False   \n",
       "12836                     False                              False   \n",
       "13411                      True                              False   \n",
       "12820                     False                              False   \n",
       "11702                      True                              False   \n",
       "\n",
       "       blood_type_AB  blood_type_B  blood_type_O  dr_00_homozygous  \\\n",
       "12405          False         False         False             False   \n",
       "12836          False         False          True             False   \n",
       "13411          False         False         False             False   \n",
       "12820          False         False         False             False   \n",
       "11702          False         False         False             False   \n",
       "\n",
       "       b_00_homozygous  a_00_homozygous    idx  \n",
       "12405             True            False  41026  \n",
       "12836            False            False  32477  \n",
       "13411            False            False  17954  \n",
       "12820            False            False  38483  \n",
       "11702            False            False  26210  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ord_test = df_test.drop(columns=['date_transplanted', 'days_waiting', 'idx'])\n",
    "\n",
    "def compare_entries(xs, y):\n",
    "    pivot = X_ord_test.loc[y]\n",
    "    pivot.index = pivot.index + '_b'\n",
    "    data = X_ord_test.loc[xs].rename(columns = lambda col: f\"{col}_a\")\n",
    "    s = []\n",
    "    for i in data.index:\n",
    "        s.append(pd.concat((data.loc[i], pivot)))\n",
    "    return forest_grid.predict_proba(pd.DataFrame(s))[:,1] >= 0.5\n",
    "\n",
    "order = qsort(X_ord_test.index, compare_entries)\n",
    "df_test.loc[order].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lifelines\n",
      "  Downloading lifelines-0.27.8-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.0 in /home/guigb/.local/lib/python3.11/site-packages (from lifelines) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/guigb/.local/lib/python3.11/site-packages (from lifelines) (1.11.3)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /home/guigb/.local/lib/python3.11/site-packages (from lifelines) (2.1.0)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /home/guigb/.local/lib/python3.11/site-packages (from lifelines) (3.8.1)\n",
      "Collecting autograd>=1.5 (from lifelines)\n",
      "  Downloading autograd-1.6.2-py3-none-any.whl.metadata (706 bytes)\n",
      "Collecting autograd-gamma>=0.3 (from lifelines)\n",
      "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting formulaic>=0.2.2 (from lifelines)\n",
      "  Downloading formulaic-0.6.6-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting future>=0.15.2 (from autograd>=1.5->lifelines)\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting astor>=0.8 (from formulaic>=0.2.2->lifelines)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n",
      "  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting typing-extensions>=4.2.0 (from formulaic>=0.2.2->lifelines)\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt>=1.0 (from formulaic>=0.2.2->lifelines)\n",
      "  Downloading wrapt-1.15.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /home/guigb/.local/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/guigb/.local/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/guigb/.local/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/guigb/.local/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/guigb/.local/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/guigb/.local/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/guigb/.local/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/guigb/.local/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/guigb/.local/lib/python3.11/site-packages (from pandas>=1.0.0->lifelines) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/guigb/.local/lib/python3.11/site-packages (from pandas>=1.0.0->lifelines) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/guigb/.local/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.16.0)\n",
      "Downloading lifelines-0.27.8-py3-none-any.whl (350 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m350.7/350.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading autograd-1.6.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading formulaic-0.6.6-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Building wheels for collected packages: autograd-gamma, future\n",
      "  Building wheel for autograd-gamma (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=1a6c7dd403139f72f1823749d7a904758932aecbba3ade0b04f3165d3d0de1fe\n",
      "  Stored in directory: /home/guigb/.cache/pip/wheels/8b/67/f4/2caaae2146198dcb824f31a303833b07b14a5ec863fb3acd7b\n",
      "  Building wheel for future (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492024 sha256=97794036aa3c0797fda7c8d018bb24d0f345a637321d911a6beb4d1018be2d39\n",
      "  Stored in directory: /home/guigb/.cache/pip/wheels/da/19/ca/9d8c44cd311a955509d7e13da3f0bea42400c469ef825b580b\n",
      "Successfully built autograd-gamma future\n",
      "Installing collected packages: wrapt, typing-extensions, interface-meta, future, astor, autograd, formulaic, autograd-gamma, lifelines\n",
      "Successfully installed astor-0.8.1 autograd-1.6.2 autograd-gamma-0.5.0 formulaic-0.6.6 future-0.18.3 interface-meta-1.3.0 lifelines-0.27.8 typing-extensions-4.8.0 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5332455388149704"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "\n",
    "order_inv = pd.Series(index = df_test.index)\n",
    "for i, j in enumerate(order):\n",
    "    order_inv.loc[j] = i\n",
    "\n",
    "concordance_index(df_test['date_transplanted'].astype(int), order_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibra√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# forest = RandomForestClassifier(n_estimators=200, max_features='log2')\n",
    "# forest.fit(X_train, y_train)\n",
    "y_test_forest_2 = forest_grid.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(y_test_forest_2.reshape(-1,1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brier score original 0.19100336631111112\n",
      "Brier score p√≥s-calibra√ß√£o 0.17752834921293678\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "pred_probs_space = np.linspace(y_test_forest_2.min(), y_test_forest_2.max(), 20)\n",
    "\n",
    "empirical_probs = []\n",
    "pred_probs_midpoints = []\n",
    "\n",
    "for i in range(len(pred_probs_space)-1):\n",
    "    empirical_probs.append(np.mean(y_test[(y_test_forest_2 > pred_probs_space[i]) & (y_test_forest_2 < pred_probs_space[i+1])]))\n",
    "    pred_probs_midpoints.append((pred_probs_space[i] + pred_probs_space[i+1])/2)\n",
    "\n",
    "\n",
    "calibrated_probs = lr.predict_proba(np.array([0.0]+pred_probs_midpoints+[1.0]).reshape(-1,1))[:,1]\n",
    "print('Brier score original', brier_score_loss(y_test, y_test_forest_2.reshape(-1,1)))\n",
    "print('Brier score p√≥s-calibra√ß√£o', brier_score_loss(y_test, lr.predict_proba(y_test_forest_2.reshape(-1,1))[:,1]))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(pred_probs_midpoints, empirical_probs, marker='o')\n",
    "plt.title(f\"Sa√≠da de probabilidades de Random Forest\", fontsize=20)\n",
    "plt.xlabel('Probabilidade predita', fontsize=14)\n",
    "plt.ylabel('Probabilidade emp√≠rica', fontsize=14)\n",
    "\n",
    "plt.plot([0,1],[0,1],linestyle='--',color='gray')\n",
    "plt.scatter([0.0]+pred_probs_midpoints+[1.0], calibrated_probs, linewidth=2, marker='x')\n",
    "\n",
    "plt.legend(['original', 'ideal', 'calibrado'], fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
